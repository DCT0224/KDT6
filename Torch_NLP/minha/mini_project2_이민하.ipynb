{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>거미막하강내에서  출혈이 일어나 뇌척수액에 피가 섞인 상태로  지주막하출혈이  라...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>원인질환으로는  중년 이후의 뇌동맥류 파열이 압도적으로 많으며  그밖에 뇌동정맥기형...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>격심한  두통 및 구역질  구토증상이 있으며 의식장애가 나타난다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>타각적으로는 목이 뻣뻣해지는  등 수막자극징후가 인정된다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>재출혈이 일어나는 경우가 많고  특히 발작 후  주  이내는 그 빈도수가 뚜렷하게...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101</th>\n",
       "      <td>속발성 월경곤란증 원인 질환을 치료하면 증상이 소실될 수 있다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>일반요법 담배는 혈액순환에 장애가 될 수 있으므로 줄이거나 끊도록 권한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2103</th>\n",
       "      <td>알코올은 혈당을 높이는 요인이 되므로 당뇨로 인한 월경곤란증을 경험하는 환자는 음주...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2104</th>\n",
       "      <td>적당한 운동과 안정  국소 온열요법은 통증을 감소시키는데 더욱 좋은 효과를 가질 수 있다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2105</th>\n",
       "      <td>통증발작시 뜨거운 물수건이나 뜨거운 물병을 이용해 찜질을 하면 통증을 어느정도 경감...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2106 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0      거미막하강내에서  출혈이 일어나 뇌척수액에 피가 섞인 상태로  지주막하출혈이  라...      1\n",
       "1     원인질환으로는  중년 이후의 뇌동맥류 파열이 압도적으로 많으며  그밖에 뇌동정맥기형...      1\n",
       "2                   격심한  두통 및 구역질  구토증상이 있으며 의식장애가 나타난다      1\n",
       "3                       타각적으로는 목이 뻣뻣해지는  등 수막자극징후가 인정된다      1\n",
       "4      재출혈이 일어나는 경우가 많고  특히 발작 후  주  이내는 그 빈도수가 뚜렷하게...      1\n",
       "...                                                 ...    ...\n",
       "2101                 속발성 월경곤란증 원인 질환을 치료하면 증상이 소실될 수 있다      0\n",
       "2102           일반요법 담배는 혈액순환에 장애가 될 수 있으므로 줄이거나 끊도록 권한다      0\n",
       "2103  알코올은 혈당을 높이는 요인이 되므로 당뇨로 인한 월경곤란증을 경험하는 환자는 음주...      0\n",
       "2104  적당한 운동과 안정  국소 온열요법은 통증을 감소시키는데 더욱 좋은 효과를 가질 수 있다      0\n",
       "2105  통증발작시 뜨거운 물수건이나 뜨거운 물병을 이용해 찜질을 하면 통증을 어느정도 경감...      0\n",
       "\n",
       "[2106 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = '../data/text_file.csv'\n",
    "\n",
    "dataDF = pd.read_csv(PATH)\n",
    "dataDF.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "dataDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataDF['text']\n",
    "target = dataDF['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,\n",
    "                                                    target,\n",
    "                                                    test_size = 0.1,\n",
    "                                                    random_state = 1,\n",
    "                                                    stratify = target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'가',\n",
       " '같',\n",
       " '것',\n",
       " '그',\n",
       " '그것',\n",
       " '그러',\n",
       " '그러나',\n",
       " '그런',\n",
       " '그렇',\n",
       " '그리고',\n",
       " '나',\n",
       " '년',\n",
       " '놓',\n",
       " '다른',\n",
       " '더',\n",
       " '데',\n",
       " '되',\n",
       " '두',\n",
       " '들',\n",
       " '등',\n",
       " '때',\n",
       " '때문',\n",
       " '또',\n",
       " '많',\n",
       " '말',\n",
       " '말하',\n",
       " '못하',\n",
       " '받',\n",
       " '번',\n",
       " '보',\n",
       " '살',\n",
       " '수',\n",
       " '시키',\n",
       " '싶',\n",
       " '아니',\n",
       " '안',\n",
       " '않',\n",
       " '알',\n",
       " '어떤',\n",
       " '어떻',\n",
       " '없',\n",
       " '오',\n",
       " '원',\n",
       " '위하',\n",
       " '이',\n",
       " '이렇',\n",
       " '일',\n",
       " '있',\n",
       " '잘',\n",
       " '점',\n",
       " '좀',\n",
       " '좋',\n",
       " '주',\n",
       " '지',\n",
       " '크',\n",
       " '하',\n",
       " '하나',\n",
       " '한'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 처리 (Spacy)\n",
    "\n",
    "TOKEN_MODEL = 'ko_core_news_lg'\n",
    "\n",
    "nlp = spacy.load(TOKEN_MODEL)\n",
    "\n",
    "# Spacy 내장 stopwords가 많지 않아, txt파일로 추가\n",
    "nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 깃허브에 공유되어 있는 한국어 불용어 리스트\n",
    "STOPWORDS_PATH = '../data/stopwords-ko.txt'\n",
    "\n",
    "with open(STOPWORDS_PATH) as f:\n",
    "    stopwords = f.read().split('\\n')\n",
    "\n",
    "# Spacy의 내장 불용어 리스트가 set이므로 중복을 신경쓰지 않고, 바로 추가\n",
    "for word in stopwords:\n",
    "    nlp.Defaults.stop_words.add(word)\n",
    "    \n",
    "    # 불용어 리스트에 단어를 넣어도, 속성 변경을 하지 않으면 불용어 처리를 하지 않음\n",
    "    nlp.vocab[word].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 데이터셋 생성\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, feature, label):\n",
    "        self.feature = feature\n",
    "        self.label = label\n",
    "        self.n_rows = feature.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_rows\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.feature.iloc[index], self.label.iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDS = TextDataset(X_train, y_train)\n",
    "testDS = TextDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 때로 무증상일  경우도 있다 0\n"
     ]
    }
   ],
   "source": [
    "for feature, label in trainDS:\n",
    "    print(feature, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateToken(dataset):\n",
    "    for text, label in dataset:\n",
    "        token_list = []\n",
    "        doc = nlp(text)\n",
    "\n",
    "        for token in doc:\n",
    "            if (not token.is_punct) and (not token.is_stop) and (not token.is_space):\n",
    "                token_list.append(str(token))\n",
    "        yield token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_generator = generateToken(trainDS)\n",
    "test_token_generator = generateToken(testDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한번에 많은 양의 데이터를 출력하면, Visual Studio가 꺼지므로, 조금만 출력하기 위한 함수\n",
    "def check_data(data):\n",
    "    i = 0\n",
    "    for _ in data:\n",
    "        print(_)\n",
    "        i += 1\n",
    "        if i == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['때로', '무증상일', '경우도']\n",
      "['전체', '인구의', '증상을', '가지고', '있으며', '여성이', '남성에', '비하여', '배이상', '많다']\n",
      "['이담제', '염증성', '산물의', '배설을', '촉진시키기위해', '종근당', '제스탄', '쓴다']\n",
      "['열이', '없어지면', '목욕이나', '샤워를', '해도', '좋다']\n",
      "['협심증', '발작을', '일으키기', '쉬우며', '때로는', '심장성', '천식을', '일으키는', '수도']\n",
      "['도', '정도의', '고열을', '동반하는', '수가', '많으며', '발열이', '일간', '지속된다']\n",
      "['문맥압항진의', '원인으로는', '간경변증', '등의', '간잘환', '문맥혈전증', '등의', '문맥질환', '비정맥혈전증', '등의', '비장질환', '등을']\n",
      "['통년성', '또는', '지속성', '알레르기염', '집먼지진드기', '가장', '많이', '생기며', '애완동물', '고양이', '개의', '상피세포', '면사류', '곡식류', '곰팡이류', '등도', '주요', '알레르겐으로', '작용한다']\n",
      "['분류', '이병변이', '신장에만', '국한된', '일차성과', '전신질환의', '일부분이니', '이차성으로', '나뉜다']\n",
      "['보통', '감기', '등의', '상기도', '감염이나', '구강내', '처치와', '같은', '일을', '후에', '나타나는', '경우가', '흔하다']\n"
     ]
    }
   ],
   "source": [
    "check_data(train_token_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freqs = {}\n",
    "\n",
    "for token_list in train_token_generator:\n",
    "    for token in token_list:\n",
    "        if token not in token_freqs:\n",
    "            token_freqs[token] = 1\n",
    "        else:\n",
    "            token_freqs[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tokens = sorted(token_freqs.items(), key = lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('한다', 198)\n",
      "('경우', 153)\n",
      "('또는', 93)\n",
      "('의한', 88)\n",
      "('증상이', 72)\n",
      "('것이', 70)\n",
      "('등의', 67)\n",
      "('등이', 65)\n",
      "('가장', 60)\n",
      "('경우에는', 59)\n"
     ]
    }
   ],
   "source": [
    "check_data(sorted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN, OOV_TOKEN = 'pad', 'oov'\n",
    "\n",
    "vocab = {PAD_TOKEN : 0, OOV_TOKEN : 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, token in enumerate(sorted_tokens, 2):\n",
    "    vocab[token[0]] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad\n",
      "oov\n",
      "한다\n",
      "경우\n",
      "또는\n",
      "의한\n",
      "증상이\n",
      "것이\n",
      "등의\n",
      "등이\n"
     ]
    }
   ],
   "source": [
    "check_data(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numbering(generator):\n",
    "    encoded_data = []\n",
    "\n",
    "    for token_list in generator:\n",
    "        L = []\n",
    "        for token in token_list:\n",
    "            if token in vocab:\n",
    "                L.append(vocab[token])\n",
    "            else:\n",
    "                L.append(vocab[OOV_TOKEN])\n",
    "        encoded_data.append(L)\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_generator = generateToken(trainDS)\n",
    "test_token_generator = generateToken(testDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = numbering(train_token_generator)\n",
    "test_ids = numbering(test_token_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1963, 1, 53]\n",
      "[494, 1, 81, 6359, 30, 1, 2305, 1, 5411, 31]\n",
      "[4788, 309, 1, 2151, 1, 1, 1, 179]\n",
      "[218, 1593, 1, 6074, 1, 97]\n",
      "[445, 1307, 1247, 4082, 212, 1, 1, 46, 43]\n",
      "[89, 607, 1, 646, 124, 2917, 1975, 115, 1107]\n",
      "[1, 285, 1348, 8, 1, 1, 8, 1, 1, 8, 1, 21]\n",
      "[2942, 4, 1201, 1509, 2329, 10, 20, 1076, 8165, 8166, 980, 8167, 8168, 8169, 8170, 525, 317, 1510, 8171]\n",
      "[106, 1, 1, 1, 1, 1, 1, 1, 694]\n",
      "[71, 464, 8, 8045, 5866, 1, 3923, 16, 1360, 138, 45, 22, 6436]\n",
      "\n",
      "[1, 315]\n",
      "[1, 1, 6795, 1723, 1, 1, 1, 868, 1, 2728, 1, 1, 1327, 1, 2]\n",
      "[2396, 1, 848, 172, 1, 1, 6154, 1, 1, 309, 102, 42]\n",
      "[2097, 1, 183, 411, 1, 1, 9306, 1, 1, 1, 1, 1, 1, 1, 6018, 1]\n",
      "[1769, 2682, 1, 5061, 7727, 997, 58, 1, 129, 2291, 1, 6436]\n",
      "[3815, 302, 1, 7108, 1129, 349, 2]\n",
      "[1507, 1, 1]\n",
      "[311, 2039, 1023, 254, 4, 272, 21, 1261, 2324, 1, 1, 883, 197, 1, 2516, 190, 1, 372, 459, 8461, 6572]\n",
      "[290, 52, 1, 178, 1161, 52, 1725, 1]\n",
      "[1891, 1, 1, 1, 1, 11, 834, 343, 3578]\n"
     ]
    }
   ],
   "source": [
    "check_data(train_ids)\n",
    "print()\n",
    "check_data(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_length = [len(data) for data in train_ids]\n",
    "test_data_length = [len(data) for data in test_ids]\n",
    "\n",
    "fig = plt.figure(figsize = (15, 15))\n",
    "axes = fig.subplots(1, 2)\n",
    "axes[0].hist(train_data_length)\n",
    "axes[1].hist(test_data_length)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 15\n",
    "\n",
    "def padding(data_ids):\n",
    "    for index, data in enumerate(data_ids):\n",
    "        current_length = len(data)\n",
    "        if current_length < MAX_LENGTH:\n",
    "            data_ids[index] = data + ([vocab[PAD_TOKEN]] * (MAX_LENGTH - current_length))\n",
    "        else:\n",
    "            data_ids[index] = data[ : MAX_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 [1963, 1, 53, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "15 [494, 1, 81, 6359, 30, 1, 2305, 1, 5411, 31, 0, 0, 0, 0, 0]\n",
      "15 [4788, 309, 1, 2151, 1, 1, 1, 179, 0, 0, 0, 0, 0, 0, 0]\n",
      "15 [218, 1593, 1, 6074, 1, 97, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "15 [445, 1307, 1247, 4082, 212, 1, 1, 46, 43, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "15 [1, 315, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "15 [1, 1, 6795, 1723, 1, 1, 1, 868, 1, 2728, 1, 1, 1327, 1, 2]\n",
      "15 [2396, 1, 848, 172, 1, 1, 6154, 1, 1, 309, 102, 42, 0, 0, 0]\n",
      "15 [2097, 1, 183, 411, 1, 1, 9306, 1, 1, 1, 1, 1, 1, 1, 6018]\n",
      "15 [1769, 2682, 1, 5061, 7727, 997, 58, 1, 129, 2291, 1, 6436, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "padding(train_ids)\n",
    "padding(test_ids)\n",
    "\n",
    "for data in train_ids[:5]:\n",
    "    print(len(data), data)\n",
    "\n",
    "print()\n",
    "\n",
    "for data in test_ids[:5]:\n",
    "    print(len(data), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 데이터셋 생성\n",
    "class TextDataset2(Dataset):\n",
    "    def __init__(self, feature, label):\n",
    "        self.feature = feature\n",
    "        self.label = label\n",
    "        self.n_rows = feature.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_rows\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.IntTensor(self.feature.iloc[index]), torch.IntTensor(self.label.iloc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치화를 시긴 데이터로 데이터셋 생성\n",
    "train_df = pd.DataFrame(train_ids)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "test_df = pd.DataFrame(test_ids)\n",
    "y_test = pd.DataFrame(y_test)\n",
    "\n",
    "train_dataset = TextDataset2(train_df, y_train)\n",
    "test_dataset = TextDataset2(test_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>315</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6795</td>\n",
       "      <td>1723</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>868</td>\n",
       "      <td>1</td>\n",
       "      <td>2728</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1327</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2396</td>\n",
       "      <td>1</td>\n",
       "      <td>848</td>\n",
       "      <td>172</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6154</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>309</td>\n",
       "      <td>102</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2097</td>\n",
       "      <td>1</td>\n",
       "      <td>183</td>\n",
       "      <td>411</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9306</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1769</td>\n",
       "      <td>2682</td>\n",
       "      <td>1</td>\n",
       "      <td>5061</td>\n",
       "      <td>7727</td>\n",
       "      <td>997</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>129</td>\n",
       "      <td>2291</td>\n",
       "      <td>1</td>\n",
       "      <td>6436</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>1</td>\n",
       "      <td>9232</td>\n",
       "      <td>1</td>\n",
       "      <td>5645</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>203</td>\n",
       "      <td>774</td>\n",
       "      <td>1</td>\n",
       "      <td>4729</td>\n",
       "      <td>1</td>\n",
       "      <td>4729</td>\n",
       "      <td>6879</td>\n",
       "      <td>1</td>\n",
       "      <td>2518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>14</td>\n",
       "      <td>9480</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>8599</td>\n",
       "      <td>8036</td>\n",
       "      <td>81</td>\n",
       "      <td>479</td>\n",
       "      <td>6066</td>\n",
       "      <td>2617</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>1</td>\n",
       "      <td>787</td>\n",
       "      <td>8958</td>\n",
       "      <td>1</td>\n",
       "      <td>7465</td>\n",
       "      <td>2122</td>\n",
       "      <td>1186</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>266</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>1</td>\n",
       "      <td>2097</td>\n",
       "      <td>2778</td>\n",
       "      <td>1</td>\n",
       "      <td>2719</td>\n",
       "      <td>1</td>\n",
       "      <td>2383</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2     3     4     5     6     7     8     9    10    11  \\\n",
       "0       1   315     0     0     0     0     0     0     0     0    0     0   \n",
       "1       1     1  6795  1723     1     1     1   868     1  2728    1     1   \n",
       "2    2396     1   848   172     1     1  6154     1     1   309  102    42   \n",
       "3    2097     1   183   411     1     1  9306     1     1     1    1     1   \n",
       "4    1769  2682     1  5061  7727   997    58     1   129  2291    1  6436   \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "206     1  9232     1  5645     1     1   203   774     1  4729    1  4729   \n",
       "207    14  9480     3   110  8599  8036    81   479  6066  2617    1     0   \n",
       "208     1   787  8958     1  7465  2122  1186     0     0     0    0     0   \n",
       "209    12     1   266     3     1     7    10  1014     0     0    0     0   \n",
       "210     1  2097  2778     1  2719     1  2383     0     0     0    0     0   \n",
       "\n",
       "       12  13    14  \n",
       "0       0   0     0  \n",
       "1    1327   1     2  \n",
       "2       0   0     0  \n",
       "3       1   1  6018  \n",
       "4       0   0     0  \n",
       "..    ...  ..   ...  \n",
       "206  6879   1  2518  \n",
       "207     0   0     0  \n",
       "208     0   0     0  \n",
       "209     0   0     0  \n",
       "210     0   0     0  \n",
       "\n",
       "[211 rows x 15 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 16, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_generator = generateToken(trainDS)\n",
    "test_token_generator = generateToken(testDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['때로', '무증상일', '경우도'],\n",
       " ['전체', '인구의', '증상을', '가지고', '있으며', '여성이', '남성에', '비하여', '배이상', '많다'],\n",
       " ['이담제', '염증성', '산물의', '배설을', '촉진시키기위해', '종근당', '제스탄', '쓴다'],\n",
       " ['열이', '없어지면', '목욕이나', '샤워를', '해도', '좋다'],\n",
       " ['협심증', '발작을', '일으키기', '쉬우며', '때로는', '심장성', '천식을', '일으키는', '수도']]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token_list = []\n",
    "\n",
    "for token in train_token_generator:\n",
    "    train_token_list.append(token)\n",
    "\n",
    "train_token_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '../models/'\n",
    "\n",
    "word2vec = Word2Vec(\n",
    "    sentences = train_token_list,\n",
    "    vector_size = 128,\n",
    "    window = 5,\n",
    "    min_count = 1,\n",
    "    sg = 0,\n",
    "    epochs = 30,\n",
    "    max_final_vocab = None\n",
    ")\n",
    "\n",
    "word2vec.save(MODEL_PATH + 'word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.0178346e-03 -1.7415009e-02  1.2734567e-02  6.9466833e-04\n",
      "  2.1052739e-02 -1.9261047e-02  1.0189073e-02 -1.0676056e-02\n",
      " -1.5981859e-02 -9.8442810e-04  1.9595018e-02  2.3723189e-03\n",
      " -6.1482098e-03 -2.3287816e-02  5.3606145e-03  1.1621552e-02\n",
      " -1.5604365e-02  4.2904443e-03 -3.4391019e-02  1.5494953e-02\n",
      "  1.3218078e-02 -6.2542415e-04 -1.7475986e-03 -3.0853201e-02\n",
      " -1.6070314e-02  1.5956949e-02 -2.0209180e-02  1.1527786e-02\n",
      " -1.2927998e-03 -3.4684024e-03 -6.6028982e-03  5.8202217e-03\n",
      "  1.1173513e-02  7.9093417e-03 -1.3327443e-02  1.1529320e-02\n",
      "  1.6077859e-02 -1.8706486e-02  2.9030039e-03  1.0756666e-03\n",
      "  5.6476854e-03  6.3179065e-03 -7.3078526e-03 -1.5530602e-02\n",
      "  2.3065804e-02 -5.8714948e-03 -5.0455811e-03 -2.8047913e-03\n",
      " -1.4408421e-02  1.0917467e-02  8.7410063e-03 -1.0043499e-02\n",
      "  1.8357314e-02  2.9392606e-02  9.3169925e-05 -3.1552424e-03\n",
      "  2.7284360e-02  6.5836376e-03 -1.9971514e-02  7.3003680e-03\n",
      " -1.9869536e-02 -3.0684713e-03 -2.2797295e-04 -5.2848379e-03\n",
      "  2.5838112e-02 -1.5914893e-02  4.5702439e-03 -7.4158172e-04\n",
      " -6.0925921e-03 -1.4766987e-02 -4.5918594e-03 -1.0767003e-02\n",
      " -2.0600641e-02 -1.0446974e-02  1.8546965e-02 -2.3369856e-02\n",
      " -1.4662166e-02 -1.3390534e-02 -2.4004990e-02  4.5964150e-03\n",
      " -1.5171740e-02 -1.0134715e-02  7.4047572e-03  2.9506071e-02\n",
      "  1.8252835e-02  2.6827622e-02  1.1723739e-02 -1.9690594e-02\n",
      "  1.0832241e-02  1.1344208e-02 -6.4725704e-03 -1.5652036e-02\n",
      "  7.1819045e-04  1.0647857e-02 -1.2450246e-04  1.4412799e-03\n",
      " -1.6550252e-02 -2.4528001e-02 -1.1078786e-03 -1.6659303e-03\n",
      " -2.5378974e-02 -3.2888926e-03 -9.2790117e-03  1.6215653e-03\n",
      "  7.5778752e-03  1.1091140e-02  6.0655433e-03  4.9014832e-04\n",
      "  1.2735945e-02 -1.5402305e-02  2.8875351e-04 -6.8522338e-03\n",
      "  2.4739953e-04  1.6441112e-02 -1.7558308e-02 -6.9520003e-03\n",
      "  2.3529013e-03  1.0175742e-02  8.8652587e-03  5.3467136e-03\n",
      " -1.6814746e-02 -2.8132822e-02 -6.8588280e-03  1.9326469e-02\n",
      " -1.6939780e-02 -1.1863267e-02  4.3360773e-03 -5.5268395e-04]\n",
      "[('방법도', 0.945762574672699), ('질병의', 0.9451616406440735), ('판막치환술을', 0.9450388550758362), ('개월', 0.9446948170661926), ('하복통', 0.9444578886032104)]\n",
      "0.89934313\n"
     ]
    }
   ],
   "source": [
    "## 생성된 모델 확인\n",
    "word2vec = Word2Vec.load(MODEL_PATH + 'word2vec.model')\n",
    "\n",
    "word = '호흡'\n",
    "\n",
    "print(word2vec.wv[word])\n",
    "print(word2vec.wv.most_similar(word, topn = 5))\n",
    "print(word2vec.wv.similarity(w1 = word, w2 = '최근'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec의 vector_size와 embedding_dim의 크기가 같아야 되는거 같다.\n",
    "\n",
    "n_vocab = len(vocab)\n",
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "n_layers = 10\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 - word2vec의 max_final_vocab을 제한해 버리면,\n",
    "# sentences 파라미터에 들어가는 토큰 수가 클 수록, 임베딩이 되지 않는 경우가 생김\n",
    "# 이번엔 제한 없이 했지만, 단어 사전 수를 제한해서 빈도 수가 적은 애들은 \n",
    "# 메모리를 생각해서 전부 oov로 전환 시키는 것이 좋을 듯 하다.\n",
    "\n",
    "init_embeddings = np.zeros((n_vocab, embedding_dim))\n",
    "\n",
    "for token, index in vocab.items():\n",
    "    if token not in [PAD_TOKEN, OOV_TOKEN]:\n",
    "        init_embeddings[index] = word2vec.wv[token]\n",
    "\n",
    "embedding_layer = nn.Embedding.from_pretrained(\n",
    "    torch.tensor(init_embeddings, dtype = torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 0.39514503 -0.99020988  0.60180229  0.2802552   1.10267842 -0.91574484\n",
      "  0.13944748 -0.29138207 -0.42729509  0.32774815  0.65812099 -0.18094005\n",
      " -0.1074506  -0.81275052  0.37217569  0.84253895 -0.83167052  0.03079611\n",
      " -1.46801603  0.73997301  1.10040545  0.30425084 -0.31608135 -1.63758349\n",
      " -1.11029339  0.58538675 -1.11628997  0.57232571  0.28277877 -0.1053634\n",
      " -0.41483203  0.35282785  0.45443901  0.74261343 -0.42812511  0.67016155\n",
      "  1.1594069  -0.64845967  0.42101482  0.45157588  0.25412434  0.54906362\n",
      " -0.1810898  -0.58501643  1.01633132 -0.34535095 -0.44211715 -0.23896283\n",
      " -0.34680659  0.5036552   0.73894852 -0.23738629  0.66807526  1.15346122\n",
      " -0.19344346 -0.25003788  1.07675612  0.00513127 -0.91224331  0.53466344\n",
      " -0.85815364 -0.11027302  0.38110909  0.00238683  1.51833141 -0.42392737\n",
      "  0.63744169  0.05466696 -0.4013384  -1.05415773 -0.00367931 -0.65778935\n",
      " -1.36535275 -0.86953008  0.59978271 -0.83016622 -0.69174969 -0.35950726\n",
      " -1.21501541  0.59953779 -0.82541502 -0.89771354  0.68681276  1.20092905\n",
      "  0.80938029  1.22634411  0.8500554  -1.03706026  0.39869902  0.9863562\n",
      " -0.71346211 -0.62640405  0.14194264  0.15408069  0.27402616 -0.01739223\n",
      " -0.68398017 -1.12166083 -0.15801148 -0.09840336 -1.58427382 -0.1329201\n",
      " -0.15887855  0.12651248  0.62303972  0.44169879  0.03085555  0.15975969\n",
      "  0.52406877 -0.41568834 -0.04829779 -0.42358708  0.04459637  0.67313349\n",
      " -0.65150446 -0.1925223   0.37446976  0.54228878  0.8588661   0.18209094\n",
      " -0.86423403 -1.06942558 -0.10694911  0.65811831 -0.53476429 -0.31473574\n",
      " -0.0310063   0.00604194]\n",
      "[ 0.50805777 -1.28560853  0.79315722  0.32834259  1.43304288 -1.19359505\n",
      "  0.17075412 -0.3768084  -0.52957076  0.4425886   0.85992211 -0.21308661\n",
      " -0.11066391 -1.05570745  0.48318961  1.08357084 -1.07794142  0.06165785\n",
      " -1.86375892  0.97115356  1.40665686  0.39682508 -0.40572879 -2.1283443\n",
      " -1.43036437  0.75933254 -1.44097924  0.75312138  0.40790543 -0.12929313\n",
      " -0.54097956  0.47689527  0.57446766  0.96971643 -0.5687415   0.8507641\n",
      "  1.49332404 -0.81381708  0.55652261  0.59237599  0.37000713  0.71980852\n",
      " -0.23043069 -0.7709952   1.31587112 -0.43475562 -0.57137328 -0.33604768\n",
      " -0.47161758  0.62265229  0.99376082 -0.33129919  0.8549729   1.50221407\n",
      " -0.25013375 -0.34317565  1.38310921 -0.00448429 -1.17462373  0.66627574\n",
      " -1.08603168 -0.1480232   0.50461644  0.02535439  1.95723343 -0.56321752\n",
      "  0.84913445  0.06276649 -0.50356567 -1.37243938 -0.03428553 -0.86888248\n",
      " -1.76115668 -1.10329664  0.75772786 -1.05889773 -0.87376714 -0.4614577\n",
      " -1.57098913  0.80619895 -1.0720396  -1.16033661  0.86854184  1.5297457\n",
      "  1.06722736  1.57932508  1.0984509  -1.35970044  0.52692926  1.28986621\n",
      " -0.94255549 -0.83196962  0.16282651  0.18057825  0.32615331 -0.0380686\n",
      " -0.86568606 -1.44339085 -0.18731681 -0.11185713 -2.04320192 -0.15237327\n",
      " -0.2013904   0.13110314  0.77601653  0.58712989  0.01825377  0.21154644\n",
      "  0.67514038 -0.51064718 -0.05998531 -0.52656573  0.05520043  0.85292405\n",
      " -0.84826803 -0.25609499  0.46507373  0.69016421  1.12289691  0.22868828\n",
      " -1.12213135 -1.38587117 -0.16356222  0.85571551 -0.70878911 -0.43369168\n",
      " -0.0573309  -0.01688923]\n",
      "[ 0.47527608 -1.18670368  0.69097     0.32041171  1.34205365 -1.11065066\n",
      "  0.18320616 -0.37608233 -0.52925694  0.43661991  0.81686801 -0.20989664\n",
      " -0.12129775 -0.96219456  0.44282806  1.03745258 -0.99830079  0.0607874\n",
      " -1.74658453  0.92958415  1.33340764  0.37670204 -0.38336715 -1.95834816\n",
      " -1.35327983  0.69938284 -1.36523807  0.67141867  0.35088122 -0.12218186\n",
      " -0.50396717  0.40113592  0.56179798  0.90656263 -0.51714993  0.8277722\n",
      "  1.40852356 -0.77441227  0.53512436  0.56484723  0.36109796  0.68199986\n",
      " -0.179409   -0.71796626  1.21205723 -0.40755895 -0.53351355 -0.32687435\n",
      " -0.42518035  0.60226923  0.92518198 -0.2907353   0.80954534  1.37185729\n",
      " -0.21039101 -0.2920785   1.28929877  0.00717752 -1.07626057  0.6314317\n",
      " -1.0254333  -0.14408229  0.44766364  0.05323054  1.84436452 -0.50673443\n",
      "  0.80795574  0.07774968 -0.50657654 -1.29378891  0.00330783 -0.81524462\n",
      " -1.64351976 -1.02066469  0.72782224 -0.99556231 -0.86257052 -0.4602716\n",
      " -1.44265592  0.76764965 -0.9763748  -1.09193254  0.81395507  1.46215427\n",
      "  0.99712044  1.47950995  1.03549552 -1.24863851  0.50274777  1.18228626\n",
      " -0.86597908 -0.75023782  0.17620306  0.174775    0.32070723 -0.02173305\n",
      " -0.82751435 -1.35205364 -0.2173581  -0.11274128 -1.89681292 -0.13887468\n",
      " -0.19970569  0.12004442  0.72458041  0.56515074  0.05976201  0.18740562\n",
      "  0.6512962  -0.48560178 -0.06355516 -0.4841997   0.05998508  0.79491669\n",
      " -0.79591811 -0.23430358  0.42546323  0.65604371  1.04438925  0.21545684\n",
      " -1.06209648 -1.27019441 -0.1435125   0.79045057 -0.67239755 -0.37988532\n",
      " -0.04057544  0.00494891]\n",
      "[ 0.4686712  -1.17852831  0.7319634   0.32747677  1.32437861 -1.10787272\n",
      "  0.13322531 -0.36872661 -0.51752269  0.38954183  0.80670172 -0.21224342\n",
      " -0.09960393 -0.97189873  0.44016284  1.00500584 -1.01589012  0.04565009\n",
      " -1.73443747  0.88885838  1.28831506  0.37778398 -0.38994208 -1.96497166\n",
      " -1.32691479  0.70139909 -1.36175692  0.67919052  0.33018729 -0.09976805\n",
      " -0.48647404  0.41564694  0.5303216   0.91475344 -0.47329128  0.81139678\n",
      "  1.37415147 -0.79166001  0.5192166   0.57594943  0.31311068  0.67419499\n",
      " -0.19763309 -0.67511022  1.18872964 -0.40585852 -0.5024246  -0.29636833\n",
      " -0.41320762  0.57217681  0.92500031 -0.30775228  0.80127728  1.37995207\n",
      " -0.2306416  -0.28850424  1.26479137 -0.00270502 -1.11688662  0.65111631\n",
      " -0.99965978 -0.14381801  0.45393741  0.02136765  1.82422078 -0.5267362\n",
      "  0.78079754  0.07306307 -0.45899555 -1.27517414 -0.02674647 -0.82378209\n",
      " -1.64948785 -1.02743638  0.71266347 -1.00698411 -0.82495534 -0.43894914\n",
      " -1.43566179  0.7182399  -0.99109453 -1.07694757  0.83007914  1.4589951\n",
      "  0.97695965  1.47528732  0.99572897 -1.21663785  0.47259226  1.16893184\n",
      " -0.85519707 -0.72017902  0.16924104  0.20460604  0.34114701 -0.03874377\n",
      " -0.82347387 -1.33978891 -0.17738727 -0.12845746 -1.8920846  -0.20762998\n",
      " -0.17049403  0.13264443  0.69960105  0.53783745  0.04008979  0.16171357\n",
      "  0.66350883 -0.50222641 -0.05133291 -0.51064062  0.09042651  0.80293244\n",
      " -0.78709841 -0.22961649  0.42071864  0.62618291  1.02821541  0.19102436\n",
      " -1.06829965 -1.25402117 -0.12440242  0.79550475 -0.670017   -0.38553017\n",
      " -0.04405653 -0.01491321]\n",
      "[ 0.32088026 -0.87244779  0.50780809  0.19596329  0.92278123 -0.7742222\n",
      "  0.0916414  -0.26618168 -0.36444995  0.27368593  0.56283283 -0.1388936\n",
      " -0.09068801 -0.69251394  0.32169101  0.69169664 -0.70477509  0.04632206\n",
      " -1.21385086  0.62874216  0.90550214  0.23598129 -0.26983365 -1.3602258\n",
      " -0.91245282  0.50307608 -0.93213004  0.47628909  0.26920816 -0.12203456\n",
      " -0.36466226  0.29108268  0.38590574  0.63126421 -0.37818795  0.57687145\n",
      "  0.97269225 -0.5379402   0.36246678  0.38415965  0.23172307  0.47700569\n",
      " -0.16939288 -0.48927051  0.87812698 -0.29509693 -0.38710031 -0.24141172\n",
      " -0.28851989  0.41478005  0.6600225  -0.2207315   0.57674348  0.98137254\n",
      " -0.18477067 -0.20690747  0.87817639 -0.02975233 -0.76051545  0.45326123\n",
      " -0.67997897 -0.07983     0.32890579  0.03671436  1.2579987  -0.35901514\n",
      "  0.56069559  0.04265915 -0.3127175  -0.91053534  0.0087014  -0.5927338\n",
      " -1.150635   -0.73093593  0.48634124 -0.6722855  -0.58025873 -0.29138285\n",
      " -0.98261958  0.54946834 -0.68632269 -0.77189499  0.6000728   1.02283478\n",
      "  0.67615914  1.02020764  0.72492492 -0.90441501  0.34902504  0.8195796\n",
      " -0.60364503 -0.53018117  0.11734782  0.11127409  0.24369545 -0.0272632\n",
      " -0.57525814 -0.94266731 -0.11470257 -0.09336736 -1.33559597 -0.09418251\n",
      " -0.15732014  0.08894165  0.53978968  0.39336157  0.0277878   0.11641428\n",
      "  0.43653005 -0.33996603 -0.01575315 -0.32869729  0.05166474  0.5511843\n",
      " -0.54985082 -0.19122078  0.30338493  0.43333763  0.74763286  0.17916115\n",
      " -0.71587628 -0.90565932 -0.12731443  0.56285667 -0.47151124 -0.29565328\n",
      " -0.03851983 -0.03106802]\n",
      "[ 4.01393086e-01 -1.05642247e+00  6.52753651e-01  2.86316603e-01\n",
      "  1.18930864e+00 -9.58145320e-01  1.20446071e-01 -3.28658551e-01\n",
      " -3.99896801e-01  3.18309158e-01  6.92226470e-01 -2.15757921e-01\n",
      " -1.62782550e-01 -8.55071247e-01  3.42810929e-01  8.95495832e-01\n",
      " -8.73986661e-01  1.95490178e-02 -1.51006639e+00  8.39905143e-01\n",
      "  1.17718089e+00  2.97264278e-01 -3.49340349e-01 -1.72141790e+00\n",
      " -1.20271158e+00  6.57779276e-01 -1.17973936e+00  5.90194643e-01\n",
      "  2.83085018e-01 -1.32297471e-01 -3.97322446e-01  3.57280433e-01\n",
      "  4.11735058e-01  8.23090255e-01 -4.18593794e-01  7.35526025e-01\n",
      "  1.22833371e+00 -6.74424529e-01  4.73422378e-01  4.68221635e-01\n",
      "  2.33977944e-01  5.93441010e-01 -1.57516047e-01 -6.37801647e-01\n",
      "  1.11971796e+00 -3.75203669e-01 -4.13415134e-01 -2.62226522e-01\n",
      " -4.06221986e-01  5.28755546e-01  7.45021284e-01 -2.51379311e-01\n",
      "  7.13945448e-01  1.23102117e+00 -1.86944723e-01 -2.75543272e-01\n",
      "  1.13400316e+00  1.57808028e-02 -9.66997802e-01  5.31072497e-01\n",
      " -8.74522626e-01 -1.49470851e-01  4.13097799e-01 -7.61581631e-03\n",
      "  1.62580824e+00 -3.92730951e-01  6.74593627e-01  8.53038505e-02\n",
      " -4.28377301e-01 -1.07702303e+00  1.23341626e-03 -7.32443810e-01\n",
      " -1.45331132e+00 -8.84150565e-01  6.15164280e-01 -8.82922411e-01\n",
      " -7.48970985e-01 -3.74740243e-01 -1.24640560e+00  6.33245111e-01\n",
      " -8.72530699e-01 -9.75520372e-01  7.21974313e-01  1.25693011e+00\n",
      "  8.41963470e-01  1.26336372e+00  9.35719490e-01 -1.10297048e+00\n",
      "  4.32251573e-01  1.01262295e+00 -7.12223053e-01 -6.89185500e-01\n",
      "  1.24665849e-01  1.39279485e-01  3.21088284e-01 -2.78069936e-02\n",
      " -7.35669255e-01 -1.18500710e+00 -1.99093670e-01 -1.12585373e-01\n",
      " -1.69870067e+00 -1.39978334e-01 -2.16738909e-01  1.51194766e-01\n",
      "  6.77472830e-01  4.99980301e-01  4.80178557e-03  1.16769731e-01\n",
      "  5.24995506e-01 -4.50564981e-01 -9.17658955e-02 -4.38407630e-01\n",
      "  4.65048552e-02  6.55570745e-01 -6.62116885e-01 -2.29181126e-01\n",
      "  4.20756221e-01  5.49526393e-01  9.05642807e-01  1.71151668e-01\n",
      " -9.22091305e-01 -1.10153544e+00 -1.25272021e-01  7.05641150e-01\n",
      " -5.51723599e-01 -3.43357712e-01  1.12050511e-02  4.16201018e-02]\n",
      "[ 0.52750373 -1.29581797  0.78709608  0.31564051  1.438411   -1.14868641\n",
      "  0.14190295 -0.40054232 -0.56062382  0.41636905  0.8443765  -0.24692407\n",
      " -0.13844991 -1.04769003  0.47366944  1.06491435 -1.08180916  0.02869662\n",
      " -1.88746333  0.94335943  1.40471208  0.37604266 -0.41331849 -2.10503101\n",
      " -1.44387579  0.75657678 -1.44512475  0.72405005  0.3728582  -0.13292989\n",
      " -0.53268379  0.47597387  0.56778735  0.97740263 -0.52806717  0.86267799\n",
      "  1.50219154 -0.81062198  0.53846174  0.60519701  0.35584009  0.69357401\n",
      " -0.22801223 -0.73279566  1.30963469 -0.44629109 -0.55565625 -0.33995858\n",
      " -0.46066901  0.63571554  0.96162504 -0.31100225  0.86180252  1.47656655\n",
      " -0.26269814 -0.30377427  1.37779331 -0.0032908  -1.17374372  0.69432855\n",
      " -1.07764506 -0.14705814  0.51740795  0.03993409  1.94748878 -0.54660672\n",
      "  0.85241425  0.05824833 -0.53681797 -1.38993478 -0.01172032 -0.87939531\n",
      " -1.75441456 -1.11090219  0.76049644 -1.0604471  -0.8945927  -0.46037358\n",
      " -1.56595647  0.76994777 -1.04985368 -1.15640616  0.88893348  1.52019453\n",
      "  1.02794302  1.58773375  1.07850039 -1.3220129   0.50929528  1.26971483\n",
      " -0.94590849 -0.79500073  0.17114662  0.17979614  0.32970303 -0.03150847\n",
      " -0.87947381 -1.44147956 -0.2291685  -0.13571005 -2.01634002 -0.16833492\n",
      " -0.21562974  0.15155438  0.80352086  0.5776242   0.03477079  0.18171878\n",
      "  0.69104946 -0.50968856 -0.07193365 -0.53215456  0.08499764  0.83184963\n",
      " -0.86410505 -0.24441485  0.43902254  0.69156325  1.12008989  0.25701773\n",
      " -1.10407543 -1.39300621 -0.1521738   0.86309594 -0.69475424 -0.379861\n",
      " -0.05473079  0.01088494]\n",
      "[ 0.41282305 -1.0193336   0.62915927  0.25817367  1.13921654 -0.94733357\n",
      "  0.12556112 -0.30253857 -0.44299567  0.3305198   0.67297596 -0.18158412\n",
      " -0.11003196 -0.84600341  0.38744736  0.86305761 -0.86659199  0.04413645\n",
      " -1.50995541  0.78770238  1.12244308  0.2880905  -0.33340043 -1.70586145\n",
      " -1.14593792  0.60300493 -1.15851295  0.57725352  0.3164129  -0.10741942\n",
      " -0.42747074  0.3851738   0.45938784  0.77041239 -0.41327581  0.71410996\n",
      "  1.19463849 -0.6484617   0.42304409  0.48471257  0.2803039   0.57802492\n",
      " -0.17491193 -0.58697557  1.06302571 -0.3453787  -0.45346859 -0.26893827\n",
      " -0.36295864  0.51537627  0.76920259 -0.2501626   0.71175653  1.20224893\n",
      " -0.19946824 -0.23853424  1.08268034  0.0186884  -0.93459225  0.53470236\n",
      " -0.87880027 -0.11825823  0.40614048  0.00904032  1.56531656 -0.42893717\n",
      "  0.6834361   0.0504977  -0.42686531 -1.10944414 -0.02301862 -0.7152943\n",
      " -1.41422343 -0.89431292  0.61845642 -0.84038764 -0.73056364 -0.39662361\n",
      " -1.2505312   0.61131299 -0.84143317 -0.94367838  0.71722388  1.23645091\n",
      "  0.84730321  1.2568084   0.8659299  -1.05622721  0.42237797  0.99430037\n",
      " -0.76310897 -0.62662053  0.15567514  0.16779503  0.25178719 -0.01301313\n",
      " -0.69487929 -1.13585281 -0.1557413  -0.12433041 -1.64607692 -0.13509552\n",
      " -0.1494492   0.12572609  0.63615626  0.46968085  0.04046004  0.12438333\n",
      "  0.54316771 -0.42470044 -0.05355707 -0.41525999  0.06334753  0.66837382\n",
      " -0.66621381 -0.21086018  0.38652131  0.53903896  0.89720738  0.19391814\n",
      " -0.89535046 -1.10266984 -0.10630817  0.66749036 -0.56800497 -0.32772988\n",
      " -0.01876322 -0.00976361]\n"
     ]
    }
   ],
   "source": [
    "check_data(init_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 분류 모델 \n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_vocab,\n",
    "                 hidden_dim,\n",
    "                 embedding_dim,\n",
    "                 n_layers,\n",
    "                 dropout = 0.5,\n",
    "                 # 양방향은 빈칸에 들어갈 단어를 고르는 등, 앞 문장과 뒷 문장이 둘 다 중요할 때 사용\n",
    "                 bidirectional = True,\n",
    "                 model_type = 'lstm',\n",
    "                 pretrained_embedding = None\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        if pretrained_embedding is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                torch.tensor(pretrained_embedding, dtype = torch.float32)\n",
    "            )\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(\n",
    "                num_embeddings = n_vocab,\n",
    "                embedding_dim = embedding_dim,\n",
    "                padding_idx = 0\n",
    "        )\n",
    "        \n",
    "        if model_type == 'rnn':\n",
    "            self.model = nn.RNN(\n",
    "                input_size = embedding_dim,\n",
    "                hidden_size = hidden_dim,\n",
    "                num_layers = n_layers,\n",
    "                bidirectional = bidirectional,\n",
    "                dropout = dropout,\n",
    "                batch_first = True\n",
    "            )\n",
    "        elif model_type == 'lstm':\n",
    "            self.model = nn.LSTM(\n",
    "                input_size = embedding_dim,\n",
    "                hidden_size = hidden_dim,\n",
    "                num_layers = n_layers,\n",
    "                bidirectional = bidirectional,\n",
    "                dropout = dropout,\n",
    "                batch_first = True\n",
    "            )\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(hidden_dim * 2, 200),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(200, 150),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(150, 100),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 50),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(50, 30),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(30, 1)\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, 200),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(200, 150),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(150, 100),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 50),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(50, 30),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(30, 1)\n",
    "            )\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        # last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return F.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SentenceClassifier(\n",
    "    n_vocab = n_vocab, hidden_dim = hidden_dim, embedding_dim = embedding_dim,\n",
    "    n_layers = n_layers, pretrained_embedding = init_embeddings\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KDP-2\\anaconda3\\envs\\TEXT_018_230_38\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 100\n",
    "LR = 0.001\n",
    "\n",
    "regLoss = nn.BCELoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = LR)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'max', patience = 100, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(featureDF, targetDF):\n",
    "    testTS = torch.LongTensor(featureDF.values).to(DEVICE)\n",
    "    targetTS = torch.FloatTensor(targetDF.values).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pre_val = classifier(testTS)\n",
    "        loss_val = regLoss(pre_val, targetTS)\n",
    "        score_val = BinaryF1Score()(pre_val, targetTS)\n",
    "        pred_labels = (pre_val >= 0.5).float()\n",
    "\n",
    "    return loss_val, score_val, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = './saved_models/'\n",
    "os.makedirs(SAVE_PATH, exist_ok = True)\n",
    "\n",
    "def training(valTS, valtargetTS):\n",
    "    BREAK_CNT_LOSS = 0\n",
    "    BREAK_CNT_SCORE = 0\n",
    "    LIMIT_VALUE = 10\n",
    "\n",
    "    LOSS_HISTORY, SCORE_HISTORY = [[], []], [[], []]\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        SAVE_MODEL = os.path.join(SAVE_PATH, f'model_{epoch}.pth')\n",
    "        SAVE_FILE = os.path.join(SAVE_PATH, f'model_weights_{epoch}.pth')\n",
    "\n",
    "        loss_total, score_total = 0, 0\n",
    "\n",
    "        for featureTS, targetTS in train_loader:\n",
    "            pre_y = classifier(featureTS)\n",
    "            targetTS = targetTS.float()\n",
    "\n",
    "            loss = regLoss(pre_y, targetTS)\n",
    "            loss_total += loss.item()\n",
    "\n",
    "            score = BinaryF1Score()(pre_y, targetTS)\n",
    "            \n",
    "            score_total += score.item()\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss, val_f1, _ = testing(valTS, valtargetTS)\n",
    "        LOSS_HISTORY[1].append(val_loss)\n",
    "        SCORE_HISTORY[1].append(val_f1)\n",
    "\n",
    "        LOSS_HISTORY[0].append(loss_total / len(train_loader))\n",
    "        SCORE_HISTORY[0].append(score_total / len(train_loader))\n",
    "\n",
    "        print(f\"[{epoch + 1}/{EPOCH}]\\n - TRAIN LOSS : {LOSS_HISTORY[0][-1]} F1 : {SCORE_HISTORY[0][-1]}\")\n",
    "        print(f\"- VAL LOSS : {LOSS_HISTORY[1][-1]} F1 : {SCORE_HISTORY[1][-1]}\")\n",
    "\n",
    "        scheduler.step(val_f1)\n",
    "\n",
    "            # 학습 진행 모니터링/스케쥴링 - 검증 DS 기준\n",
    "        # Score 기준\n",
    "        if len(SCORE_HISTORY[1]) >= 2:\n",
    "            if SCORE_HISTORY[1][-1] >= SCORE_HISTORY[1][-2]: BREAK_CNT_SCORE += 1\n",
    "\n",
    "        # 성능이 좋은 학습 가중치 저장\n",
    "\n",
    "        if len(SCORE_HISTORY[1]) == 1:\n",
    "            # 첫 번째라서 무조건 모델 파라미터 저장\n",
    "            torch.save(classifier.state_dict(), SAVE_FILE)\n",
    "            # 모델 전체 저장\n",
    "            torch.save(classifier, SAVE_MODEL)\n",
    "        else:\n",
    "            if SCORE_HISTORY[1][-1] > max(SCORE_HISTORY[1][:-1]):\n",
    "                torch.save(classifier.state_dict(), SAVE_FILE)\n",
    "                torch.save(classifier, SAVE_MODEL)\n",
    "\n",
    "        if BREAK_CNT_LOSS > LIMIT_VALUE:\n",
    "            print(f'성능 및 손실 개선이 없어서 {epoch} EPOCH에 학습 중단')\n",
    "            break\n",
    "\n",
    "    return LOSS_HISTORY, SCORE_HISTORY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100]\n",
      " - TRAIN LOSS : 0.6930702050526937 F1 : 0.4762906855593125\n",
      "- VAL LOSS : 0.6928520202636719 F1 : 0.6350364685058594\n",
      "[2/100]\n",
      " - TRAIN LOSS : 0.6917138020197551 F1 : 0.637225001056989\n",
      "- VAL LOSS : 0.6941144466400146 F1 : 0.644518256187439\n",
      "[3/100]\n",
      " - TRAIN LOSS : 0.6903645793596903 F1 : 0.6852069278558095\n",
      "- VAL LOSS : 0.6935572624206543 F1 : 0.6708860993385315\n",
      "[4/100]\n",
      " - TRAIN LOSS : 0.6922850410143534 F1 : 0.6677424391110738\n",
      "- VAL LOSS : 0.6917057037353516 F1 : 0.6132404208183289\n",
      "[5/100]\n",
      " - TRAIN LOSS : 0.6904404679934184 F1 : 0.6668258746465047\n",
      "- VAL LOSS : 0.6912016868591309 F1 : 0.662162184715271\n",
      "[6/100]\n",
      " - TRAIN LOSS : 0.6888196011384328 F1 : 0.6578293859958648\n",
      "- VAL LOSS : 0.6935476660728455 F1 : 0.6689895391464233\n",
      "[7/100]\n",
      " - TRAIN LOSS : 0.6861846625804902 F1 : 0.6589838514725367\n",
      "- VAL LOSS : 0.6926062107086182 F1 : 0.6394051909446716\n",
      "[8/100]\n",
      " - TRAIN LOSS : 0.6912880301475525 F1 : 0.6081008682648341\n",
      "- VAL LOSS : 0.6947083473205566 F1 : 0.6090225577354431\n",
      "[9/100]\n",
      " - TRAIN LOSS : 0.6901258885860443 F1 : 0.6178813278675079\n",
      "- VAL LOSS : 0.687130331993103 F1 : 0.5577689409255981\n",
      "[10/100]\n",
      " - TRAIN LOSS : 0.6902213235696156 F1 : 0.6309382677078247\n",
      "- VAL LOSS : 0.6929783821105957 F1 : 0.514018714427948\n",
      "[11/100]\n",
      " - TRAIN LOSS : 0.6928017914295197 F1 : 0.5398537516593933\n",
      "- VAL LOSS : 0.692298173904419 F1 : 0.6319702863693237\n",
      "[12/100]\n",
      " - TRAIN LOSS : 0.6912215312321981 F1 : 0.6467747390270233\n",
      "- VAL LOSS : 0.6919158101081848 F1 : 0.6688103079795837\n",
      "[13/100]\n",
      " - TRAIN LOSS : 0.6887087563673655 F1 : 0.6808403571446736\n",
      "- VAL LOSS : 0.6971228122711182 F1 : 0.6883116960525513\n",
      "[14/100]\n",
      " - TRAIN LOSS : 0.6886172473430634 F1 : 0.6760165691375732\n",
      "- VAL LOSS : 0.6901702284812927 F1 : 0.6666666865348816\n",
      "[15/100]\n",
      " - TRAIN LOSS : 0.6893995424111684 F1 : 0.6526202340920766\n",
      "- VAL LOSS : 0.6883441805839539 F1 : 0.6348122954368591\n",
      "[16/100]\n",
      " - TRAIN LOSS : 0.688946004708608 F1 : 0.6702415664990743\n",
      "- VAL LOSS : 0.6886802315711975 F1 : 0.6264150738716125\n",
      "[17/100]\n",
      " - TRAIN LOSS : 0.6920482317606608 F1 : 0.6161898722251257\n",
      "- VAL LOSS : 0.6889130473136902 F1 : 0.646258533000946\n",
      "[18/100]\n",
      " - TRAIN LOSS : 0.6898092130819956 F1 : 0.6714285641908646\n",
      "- VAL LOSS : 0.6871418952941895 F1 : 0.6730769276618958\n",
      "[19/100]\n",
      " - TRAIN LOSS : 0.6897883395353953 F1 : 0.6737869163354238\n",
      "- VAL LOSS : 0.6888344287872314 F1 : 0.6688524484634399\n",
      "[20/100]\n",
      " - TRAIN LOSS : 0.6896871447563171 F1 : 0.6667180359363556\n",
      "- VAL LOSS : 0.6863158941268921 F1 : 0.6007326245307922\n",
      "[21/100]\n",
      " - TRAIN LOSS : 0.6902458528677623 F1 : 0.6200554619232813\n",
      "- VAL LOSS : 0.6867774128913879 F1 : 0.6225680708885193\n",
      "[22/100]\n",
      " - TRAIN LOSS : 0.689397915204366 F1 : 0.6205687999725342\n",
      "- VAL LOSS : 0.6870418787002563 F1 : 0.6028369069099426\n",
      "[23/100]\n",
      " - TRAIN LOSS : 0.6860790173212687 F1 : 0.6245643635590871\n",
      "- VAL LOSS : 0.6892052292823792 F1 : 0.5339366793632507\n",
      "[24/100]\n",
      " - TRAIN LOSS : 0.689891924460729 F1 : 0.5908994714419047\n",
      "- VAL LOSS : 0.6854584217071533 F1 : 0.6549295783042908\n",
      "[25/100]\n",
      " - TRAIN LOSS : 0.6906047463417053 F1 : 0.6386333545049031\n",
      "- VAL LOSS : 0.6886148452758789 F1 : 0.6597222089767456\n",
      "[26/100]\n",
      " - TRAIN LOSS : 0.6879127641518911 F1 : 0.6744696736335755\n",
      "- VAL LOSS : 0.6874281764030457 F1 : 0.6334519386291504\n",
      "[27/100]\n",
      " - TRAIN LOSS : 0.6885867456595103 F1 : 0.6428410242001216\n",
      "- VAL LOSS : 0.6876199841499329 F1 : 0.6037735939025879\n",
      "[28/100]\n",
      " - TRAIN LOSS : 0.6900378942489624 F1 : 0.6645142406225204\n",
      "- VAL LOSS : 0.6808800101280212 F1 : 0.5931559205055237\n",
      "[29/100]\n",
      " - TRAIN LOSS : 0.687375154097875 F1 : 0.6238344689210256\n",
      "- VAL LOSS : 0.6846510767936707 F1 : 0.5702127814292908\n",
      "[30/100]\n",
      " - TRAIN LOSS : 0.6858485261599223 F1 : 0.6078333338101705\n",
      "- VAL LOSS : 0.6813140511512756 F1 : 0.5882353186607361\n",
      "[31/100]\n",
      " - TRAIN LOSS : 0.684578017393748 F1 : 0.5914228270451228\n",
      "- VAL LOSS : 0.6781954169273376 F1 : 0.5769230723381042\n",
      "[32/100]\n",
      " - TRAIN LOSS : 0.690514459212621 F1 : 0.5290008574724198\n",
      "- VAL LOSS : 0.6904446482658386 F1 : 0.4615384638309479\n",
      "[33/100]\n",
      " - TRAIN LOSS : 0.6900816321372986 F1 : 0.6203937957684199\n",
      "- VAL LOSS : 0.6824535131454468 F1 : 0.6733333468437195\n",
      "[34/100]\n",
      " - TRAIN LOSS : 0.6882759908835093 F1 : 0.6272024055322011\n",
      "- VAL LOSS : 0.6853607296943665 F1 : 0.52173912525177\n",
      "[35/100]\n",
      " - TRAIN LOSS : 0.6880852560202281 F1 : 0.6043341825405757\n",
      "- VAL LOSS : 0.6800327301025391 F1 : 0.6463878154754639\n",
      "[36/100]\n",
      " - TRAIN LOSS : 0.6863075574239095 F1 : 0.6465856790542602\n",
      "- VAL LOSS : 0.6805898547172546 F1 : 0.6377952694892883\n",
      "[37/100]\n",
      " - TRAIN LOSS : 0.6887141386667888 F1 : 0.6261544813712437\n",
      "- VAL LOSS : 0.6816405653953552 F1 : 0.6274510025978088\n",
      "[38/100]\n",
      " - TRAIN LOSS : 0.6841926713784535 F1 : 0.6306548496087392\n",
      "- VAL LOSS : 0.6774588227272034 F1 : 0.6337448358535767\n",
      "[39/100]\n",
      " - TRAIN LOSS : 0.6873029847939809 F1 : 0.6101211259762446\n",
      "- VAL LOSS : 0.6835082173347473 F1 : 0.626086950302124\n",
      "[40/100]\n",
      " - TRAIN LOSS : 0.6883842368920644 F1 : 0.6001768887043\n",
      "- VAL LOSS : 0.6829010248184204 F1 : 0.514018714427948\n",
      "[41/100]\n",
      " - TRAIN LOSS : 0.6868677457173665 F1 : 0.5753156344095866\n",
      "- VAL LOSS : 0.6828860640525818 F1 : 0.5555555820465088\n",
      "[42/100]\n",
      " - TRAIN LOSS : 0.6866748571395874 F1 : 0.5844959984223048\n",
      "- VAL LOSS : 0.6824935078620911 F1 : 0.507317066192627\n",
      "[43/100]\n",
      " - TRAIN LOSS : 0.6865496138731638 F1 : 0.5954906245072683\n",
      "- VAL LOSS : 0.6820129156112671 F1 : 0.632478654384613\n",
      "[44/100]\n",
      " - TRAIN LOSS : 0.6887550771236419 F1 : 0.6134621381759644\n",
      "- VAL LOSS : 0.6860286593437195 F1 : 0.5675675868988037\n",
      "[45/100]\n",
      " - TRAIN LOSS : 0.6866068641344706 F1 : 0.5781528721253077\n",
      "- VAL LOSS : 0.68398517370224 F1 : 0.5370370149612427\n",
      "[46/100]\n",
      " - TRAIN LOSS : 0.685708757241567 F1 : 0.598501647512118\n",
      "- VAL LOSS : 0.6826679706573486 F1 : 0.5774058699607849\n",
      "[47/100]\n",
      " - TRAIN LOSS : 0.688323958714803 F1 : 0.5730109026034673\n",
      "- VAL LOSS : 0.6846330165863037 F1 : 0.5023696422576904\n",
      "[48/100]\n",
      " - TRAIN LOSS : 0.6829158465067545 F1 : 0.588767624894778\n",
      "- VAL LOSS : 0.6813873648643494 F1 : 0.5177664756774902\n",
      "[49/100]\n",
      " - TRAIN LOSS : 0.6858115057150523 F1 : 0.5975359131892523\n",
      "- VAL LOSS : 0.6902405619621277 F1 : 0.5495495200157166\n",
      "[50/100]\n",
      " - TRAIN LOSS : 0.6863501091798146 F1 : 0.582521187265714\n",
      "- VAL LOSS : 0.6862857341766357 F1 : 0.5263158082962036\n",
      "[51/100]\n",
      " - TRAIN LOSS : 0.6859500209490458 F1 : 0.5902231117089589\n",
      "- VAL LOSS : 0.6840219497680664 F1 : 0.5327102541923523\n",
      "[52/100]\n",
      " - TRAIN LOSS : 0.6842978815237681 F1 : 0.6061922758817673\n",
      "- VAL LOSS : 0.6855164766311646 F1 : 0.5\n",
      "[53/100]\n",
      " - TRAIN LOSS : 0.6880984346071879 F1 : 0.5561939706405004\n",
      "- VAL LOSS : 0.6804698705673218 F1 : 0.49000000953674316\n",
      "[54/100]\n",
      " - TRAIN LOSS : 0.6849297523498535 F1 : 0.5904837032159169\n",
      "- VAL LOSS : 0.6856709718704224 F1 : 0.5645933151245117\n",
      "[55/100]\n",
      " - TRAIN LOSS : 0.6861132343610128 F1 : 0.5505526413520178\n",
      "- VAL LOSS : 0.6951688528060913 F1 : 0.3957219123840332\n",
      "[56/100]\n",
      " - TRAIN LOSS : 0.6852238953113556 F1 : 0.5609579225381215\n",
      "- VAL LOSS : 0.6766785383224487 F1 : 0.5840708017349243\n",
      "[57/100]\n",
      " - TRAIN LOSS : 0.6860941052436829 F1 : 0.5787537366151809\n",
      "- VAL LOSS : 0.6848523616790771 F1 : 0.5381165742874146\n",
      "[58/100]\n",
      " - TRAIN LOSS : 0.6836940387884776 F1 : 0.6189938267072042\n",
      "- VAL LOSS : 0.6862929463386536 F1 : 0.5398229956626892\n",
      "[59/100]\n",
      " - TRAIN LOSS : 0.6860315541426341 F1 : 0.6067766646544138\n",
      "- VAL LOSS : 0.6845242977142334 F1 : 0.5247524976730347\n",
      "[60/100]\n",
      " - TRAIN LOSS : 0.681499429543813 F1 : 0.6026243855555852\n",
      "- VAL LOSS : 0.6804143786430359 F1 : 0.551111102104187\n",
      "[61/100]\n",
      " - TRAIN LOSS : 0.6882399260997772 F1 : 0.5141957034667333\n",
      "- VAL LOSS : 0.6818171739578247 F1 : 0.5128205418586731\n",
      "[62/100]\n",
      " - TRAIN LOSS : 0.6837547898292542 F1 : 0.5926798899968465\n",
      "- VAL LOSS : 0.68248051404953 F1 : 0.5277777910232544\n",
      "[63/100]\n",
      " - TRAIN LOSS : 0.683848214149475 F1 : 0.5677802950143814\n",
      "- VAL LOSS : 0.6802117228507996 F1 : 0.48571428656578064\n",
      "[64/100]\n",
      " - TRAIN LOSS : 0.6841145634651185 F1 : 0.5955730189879735\n",
      "- VAL LOSS : 0.6767910122871399 F1 : 0.5844748616218567\n",
      "[65/100]\n",
      " - TRAIN LOSS : 0.6824303050835927 F1 : 0.5900422056516011\n",
      "- VAL LOSS : 0.685483455657959 F1 : 0.38418078422546387\n",
      "[66/100]\n",
      " - TRAIN LOSS : 0.682705823580424 F1 : 0.5790363391240437\n",
      "- VAL LOSS : 0.6769273281097412 F1 : 0.5953488349914551\n",
      "[67/100]\n",
      " - TRAIN LOSS : 0.6801063776016235 F1 : 0.6053152869145075\n",
      "- VAL LOSS : 0.6823192238807678 F1 : 0.4559585452079773\n",
      "[68/100]\n",
      " - TRAIN LOSS : 0.683449524641037 F1 : 0.5735677540302276\n",
      "- VAL LOSS : 0.6955423951148987 F1 : 0.3294117748737335\n",
      "[69/100]\n",
      " - TRAIN LOSS : 0.6860655605792999 F1 : 0.565493976076444\n",
      "- VAL LOSS : 0.693783700466156 F1 : 0.47926267981529236\n",
      "[70/100]\n",
      " - TRAIN LOSS : 0.6831871767838796 F1 : 0.5836913496255874\n",
      "- VAL LOSS : 0.6831192970275879 F1 : 0.46875\n",
      "[71/100]\n",
      " - TRAIN LOSS : 0.6789105415344239 F1 : 0.5771228204170863\n",
      "- VAL LOSS : 0.6872105598449707 F1 : 0.4771573543548584\n",
      "[72/100]\n",
      " - TRAIN LOSS : 0.6781651834646861 F1 : 0.5628398646910985\n",
      "- VAL LOSS : 0.6845995187759399 F1 : 0.5\n",
      "[73/100]\n",
      " - TRAIN LOSS : 0.6757429897785187 F1 : 0.5884180098772049\n",
      "- VAL LOSS : 0.6965031027793884 F1 : 0.41081079840660095\n",
      "[74/100]\n",
      " - TRAIN LOSS : 0.681354018052419 F1 : 0.5730276614427566\n",
      "- VAL LOSS : 0.6894863247871399 F1 : 0.4752475321292877\n",
      "[75/100]\n",
      " - TRAIN LOSS : 0.6788749098777771 F1 : 0.5547822723786037\n",
      "- VAL LOSS : 0.6835938692092896 F1 : 0.3333333432674408\n",
      "[76/100]\n",
      " - TRAIN LOSS : 0.677016657590866 F1 : 0.5349285791317622\n",
      "- VAL LOSS : 0.6844805479049683 F1 : 0.3544303774833679\n",
      "[77/100]\n",
      " - TRAIN LOSS : 0.6778227786223093 F1 : 0.54982557396094\n",
      "- VAL LOSS : 0.6847650408744812 F1 : 0.4134078323841095\n",
      "[78/100]\n",
      " - TRAIN LOSS : 0.6774185756842296 F1 : 0.567372206846873\n",
      "- VAL LOSS : 0.6923885345458984 F1 : 0.42937853932380676\n",
      "[79/100]\n",
      " - TRAIN LOSS : 0.6741997996966044 F1 : 0.5672249774138133\n",
      "- VAL LOSS : 0.6738402843475342 F1 : 0.4581005573272705\n",
      "[80/100]\n",
      " - TRAIN LOSS : 0.6717227359612783 F1 : 0.5461029519637426\n",
      "- VAL LOSS : 0.6764337420463562 F1 : 0.42603549361228943\n",
      "[81/100]\n",
      " - TRAIN LOSS : 0.669942686955134 F1 : 0.5414135267337163\n",
      "- VAL LOSS : 0.680683970451355 F1 : 0.4117647111415863\n",
      "[82/100]\n",
      " - TRAIN LOSS : 0.6678170979022979 F1 : 0.531078361471494\n",
      "- VAL LOSS : 0.6884405612945557 F1 : 0.37735849618911743\n",
      "[83/100]\n",
      " - TRAIN LOSS : 0.674528706073761 F1 : 0.5431921035051346\n",
      "- VAL LOSS : 0.6848577260971069 F1 : 0.39263802766799927\n",
      "[84/100]\n",
      " - TRAIN LOSS : 0.6713768740495046 F1 : 0.5344680935144425\n",
      "- VAL LOSS : 0.6871234774589539 F1 : 0.3658536672592163\n",
      "[85/100]\n",
      " - TRAIN LOSS : 0.6705910086631774 F1 : 0.5077166805664698\n",
      "- VAL LOSS : 0.6916326284408569 F1 : 0.4252873659133911\n",
      "[86/100]\n",
      " - TRAIN LOSS : 0.6772548437118531 F1 : 0.5164123088121414\n",
      "- VAL LOSS : 0.6893143653869629 F1 : 0.38823530077934265\n",
      "[87/100]\n",
      " - TRAIN LOSS : 0.6703859567642212 F1 : 0.5520560512940089\n",
      "- VAL LOSS : 0.6926965117454529 F1 : 0.3604651093482971\n",
      "[88/100]\n",
      " - TRAIN LOSS : 0.6697717626889547 F1 : 0.554037884871165\n",
      "- VAL LOSS : 0.6884973645210266 F1 : 0.371257483959198\n",
      "[89/100]\n",
      " - TRAIN LOSS : 0.6693191071351369 F1 : 0.5258065313100815\n",
      "- VAL LOSS : 0.681218147277832 F1 : 0.37662336230278015\n",
      "[90/100]\n",
      " - TRAIN LOSS : 0.662933099269867 F1 : 0.5251326123873393\n",
      "- VAL LOSS : 0.6926174163818359 F1 : 0.3164556920528412\n",
      "[91/100]\n",
      " - TRAIN LOSS : 0.6514555017153422 F1 : 0.5538867394129435\n",
      "- VAL LOSS : 0.7052253484725952 F1 : 0.3439490497112274\n",
      "[92/100]\n",
      " - TRAIN LOSS : 0.6638938824335734 F1 : 0.5286892235279084\n",
      "- VAL LOSS : 0.7009214162826538 F1 : 0.4228571355342865\n",
      "[93/100]\n",
      " - TRAIN LOSS : 0.6536050120989482 F1 : 0.5568933208783468\n",
      "- VAL LOSS : 0.6832811236381531 F1 : 0.43529412150382996\n",
      "[94/100]\n",
      " - TRAIN LOSS : 0.6610946953296661 F1 : 0.5475052932898203\n",
      "- VAL LOSS : 0.7019047737121582 F1 : 0.36781609058380127\n",
      "[95/100]\n",
      " - TRAIN LOSS : 0.6602757592995961 F1 : 0.5358216494321824\n",
      "- VAL LOSS : 0.6926330924034119 F1 : 0.4114285707473755\n",
      "[96/100]\n",
      " - TRAIN LOSS : 0.6588510652383168 F1 : 0.5761650810639064\n",
      "- VAL LOSS : 0.6852650046348572 F1 : 0.43820226192474365\n",
      "[97/100]\n",
      " - TRAIN LOSS : 0.6470586717128753 F1 : 0.5735066036383311\n",
      "- VAL LOSS : 0.6969372630119324 F1 : 0.39534884691238403\n",
      "[98/100]\n",
      " - TRAIN LOSS : 0.6581497907638549 F1 : 0.5502287904421489\n",
      "- VAL LOSS : 0.6918261647224426 F1 : 0.40236687660217285\n",
      "[99/100]\n",
      " - TRAIN LOSS : 0.6448015193144481 F1 : 0.5680060078700383\n",
      "- VAL LOSS : 0.7195674777030945 F1 : 0.2535211145877838\n",
      "[100/100]\n",
      " - TRAIN LOSS : 0.6475977003574371 F1 : 0.5390034327904384\n",
      "- VAL LOSS : 0.6785157322883606 F1 : 0.4161849617958069\n"
     ]
    }
   ],
   "source": [
    "loss, f1 = training(test_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "for feature, label in train_loader:\n",
    "    print((classifier(feature) >= 0.5).float())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEXT_018_230_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
