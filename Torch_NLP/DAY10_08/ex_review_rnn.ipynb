{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 분류 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        hidden_dim,\n",
    "        embedding_dim,\n",
    "        n_layers,\n",
    "        dropout = 0.5,\n",
    "        bidirectional = True,\n",
    "        model_type = 'lstm'):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        if model_type == 'rnn':\n",
    "            self.model = nn.RNN(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        elif model_type == 'lstm':\n",
    "            self.model = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            )\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim * 2,1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim,1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output,_ = self.model(embeddings)\n",
    "        last_output = output[:,-1,:]\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터세트 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-14\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-14\\Korpora\\nsmc\\ratings_test.txt\n",
      "Traing Data Size :  45000\n",
      "Test Data Size :  5000\n"
     ]
    }
   ],
   "source": [
    "corpus = Korpora.load('nsmc')\n",
    "corpus_df = pd.DataFrame(corpus.test)\n",
    "\n",
    "train = corpus_df.sample(frac=0.9, random_state=42)\n",
    "test = corpus_df.drop(train.index)\n",
    "\n",
    "# print(train.head(5).to_markdown()) ??\n",
    "print(\"Traing Data Size : \", len(train))\n",
    "print(\"Test Data Size : \", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 토큰화 및 단어사전 구축 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab\n",
    "\n",
    "# 토큰화\n",
    "tokenizer = Okt()\n",
    "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
    "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
    "\n",
    "# 토큰화 된걸 사전으로 만듬\n",
    "vocab = build_vocab(corpus = train_tokens,n_vocab=5000,special_tokens=[\"<pad>\",\"<unk>\"])\n",
    "\n",
    "# 맵핑 부분분\n",
    "token_to_id = {token : idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx : token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['모든',\n",
       "  '편견',\n",
       "  '을',\n",
       "  '날려',\n",
       "  '버리는',\n",
       "  '가슴',\n",
       "  '따뜻한',\n",
       "  '영화',\n",
       "  '.',\n",
       "  '로버트',\n",
       "  '드',\n",
       "  '니',\n",
       "  '로',\n",
       "  ',',\n",
       "  '필립',\n",
       "  '세이모어',\n",
       "  '호프만',\n",
       "  '영원하라',\n",
       "  '.']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정수 인코딩 및 패딩 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
      " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
      "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_sequences(sequences,max_length, pad_value):\n",
    "    result = list()\n",
    "    for sequence in sequences:\n",
    "        sequence = sequence[:max_length]\n",
    "        pad_length = max_length - len(sequence)\n",
    "        padded_sequence = sequence + [pad_value] * pad_length\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)\n",
    "\n",
    "unk_id = token_to_id['<unk>']\n",
    "train_ids = [[token_to_id.get(token,unk_id)for token in review]for review in train_tokens]\n",
    "test_ids = [[token_to_id.get(token,unk_id)for token in review]for review in test_tokens]\n",
    "\n",
    "max_length = 32\n",
    "pad_id = token_to_id['<pad>']\n",
    "train_ids = pad_sequences(train_ids,max_length,pad_id)\n",
    "test_ids = pad_sequences(test_ids,max_length,pad_id)\n",
    "\n",
    "print(train_ids[0])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로더 적용 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ids = torch.tensor(train_ids)\n",
    "test_ids = torch.tensor(test_ids)\n",
    "\n",
    "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_ids, train_labels)\n",
    "test_dataset = TensorDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실 함수와 최적화 함수 정의 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "n_vocab  = len(token_to_id)\n",
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "classifier = SentenceClassifier(\n",
    "    n_vocab=n_vocab,hidden_dim=hidden_dim,embedding_dim=embedding_dim,n_layers=n_layers\n",
    ").to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(),lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습 및 테스트 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,datasets,criterion,optimizer,device,interval):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "\n",
    "    for step,(input_ids,labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits,labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval ==0:\n",
    "            print(f'Train Loss {step} : {np.mean(losses)}')\n",
    "\n",
    "def test(model,datasets,criterion,device):\n",
    "    model.eval()\n",
    "    losses = list()\n",
    "    corrects = list()\n",
    "\n",
    "    for step, (input_ids,labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits,labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat = torch.sigmoid(logits)>.5\n",
    "        corrects.extend(\n",
    "            torch.eq(yhat,labels).cpu().tolist()\n",
    "        )\n",
    "    print(f'Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.6927856802940369\n",
      "Train Loss 500 : 0.6938404606249994\n",
      "Train Loss 1000 : 0.6865301315898781\n",
      "Train Loss 1500 : 0.6721761725808206\n",
      "Train Loss 2000 : 0.6626201713609433\n",
      "Train Loss 2500 : 0.6550581421746297\n",
      "Val Loss : 0.6011598525336757, Val Accuracy : 0.6978\n",
      "Train Loss 0 : 0.4339098632335663\n",
      "Train Loss 500 : 0.5744141944630179\n",
      "Train Loss 1000 : 0.5592193121200317\n",
      "Train Loss 1500 : 0.5391078550644035\n",
      "Train Loss 2000 : 0.5238093916235538\n",
      "Train Loss 2500 : 0.5139217551042442\n",
      "Val Loss : 0.45824491520659233, Val Accuracy : 0.7834\n",
      "Train Loss 0 : 0.4779515266418457\n",
      "Train Loss 500 : 0.41523371975935863\n",
      "Train Loss 1000 : 0.41677073037975676\n",
      "Train Loss 1500 : 0.41582335488030625\n",
      "Train Loss 2000 : 0.4114315731824189\n",
      "Train Loss 2500 : 0.4093450354468055\n",
      "Val Loss : 0.417715231855266, Val Accuracy : 0.8128\n",
      "Train Loss 0 : 0.5705158114433289\n",
      "Train Loss 500 : 0.3585417429487148\n",
      "Train Loss 1000 : 0.36271222055955726\n",
      "Train Loss 1500 : 0.3651913345321705\n",
      "Train Loss 2000 : 0.36353966742865684\n",
      "Train Loss 2500 : 0.3632579462387499\n",
      "Val Loss : 0.39996394150839826, Val Accuracy : 0.8224\n",
      "Train Loss 0 : 0.7833093404769897\n",
      "Train Loss 500 : 0.32138786339266096\n",
      "Train Loss 1000 : 0.3278916484297632\n",
      "Train Loss 1500 : 0.3283433861494422\n",
      "Train Loss 2000 : 0.32720361703607453\n",
      "Train Loss 2500 : 0.32850251687181947\n",
      "Val Loss : 0.3930276346187622, Val Accuracy : 0.8238\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier,train_loader,criterion,optimizer,device,interval)\n",
    "    test(classifier,test_loader,criterion,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습된 모델로부터 임베딩 추출 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보고싶다 [-2.13147354e+00 -9.73338604e-01 -7.23130524e-01  8.38033617e-01\n",
      "  2.92781144e-01  4.16226745e-01  1.17933583e-02 -1.00700533e+00\n",
      " -2.69620836e-01  7.94402435e-02  5.83757579e-01  7.15165377e-01\n",
      " -5.73228717e-01  1.74757552e+00 -1.28089988e+00  2.19189852e-01\n",
      " -1.48195565e+00  6.85004532e-01 -1.71868369e-01  1.21109508e-01\n",
      "  6.44406319e-01  1.33566356e+00  6.51631832e-01  5.33246994e-01\n",
      "  2.14045838e-01 -1.61191058e+00  7.25900084e-02 -1.36394358e+00\n",
      " -4.77418393e-01  7.22214222e-01  3.01264822e-01 -1.42810106e-01\n",
      "  2.77615190e-01 -9.24553692e-01 -6.60686970e-01 -8.07152867e-01\n",
      " -1.94810987e+00  7.05322981e-01 -9.50230122e-01 -1.23959124e+00\n",
      "  1.25406277e+00 -1.51167369e+00  9.22580957e-01 -1.43614089e+00\n",
      " -1.67827860e-01 -6.22930765e-01 -1.26506555e+00  5.57834804e-02\n",
      " -1.46782899e+00  9.10904408e-01 -8.59702826e-01 -4.51323569e-01\n",
      " -1.33559191e+00 -1.22612394e-01  1.56435668e-01  1.49809372e+00\n",
      " -8.72326076e-01  2.04337025e+00 -1.29371965e+00 -8.08906615e-01\n",
      "  1.34814525e+00 -1.21856058e+00 -1.28074139e-01  5.61731644e-02\n",
      "  4.06210348e-02 -1.44087744e+00 -1.63736629e+00 -9.53676939e-01\n",
      " -9.22883451e-01 -1.70148993e+00 -7.62362838e-01  1.07465637e+00\n",
      "  5.30396879e-01  7.47170091e-01  8.47846270e-01  8.62280577e-02\n",
      " -7.62716472e-01 -2.47133285e-01  3.87879908e-01  1.88156962e-01\n",
      " -5.22108734e-01  2.04134440e+00  7.93045461e-01 -5.84888041e-01\n",
      " -9.86454606e-01 -1.08824931e-01 -9.06410575e-01 -9.49920833e-01\n",
      "  6.64379358e-01  6.14499986e-01  3.01784563e+00 -6.98690176e-01\n",
      " -1.81096509e-01 -6.68391347e-01 -3.51150513e-01 -1.25022995e+00\n",
      " -7.38519877e-02 -4.51348811e-01 -1.84829429e-01 -1.01262975e+00\n",
      " -1.75756669e+00 -4.83454227e-01 -1.29229724e+00 -3.63674104e-01\n",
      "  3.12066525e-01 -1.08086527e-03  1.35312986e+00 -8.91721725e-01\n",
      "  8.77409220e-01 -1.65604770e+00 -4.23764437e-01  6.67619526e-01\n",
      " -7.00000465e-01 -2.54950500e+00 -4.86175328e-01  6.77687109e-01\n",
      " -1.45269823e+00 -1.67216673e-01 -7.95850754e-01  1.02428222e+00\n",
      "  9.87764955e-01 -1.58760452e+00 -2.70266533e-01 -1.44759798e+00\n",
      " -4.22344446e-01  7.22918808e-01  3.65687400e-01  1.47070706e+00]\n"
     ]
    }
   ],
   "source": [
    "token_to_embedding = dict()\n",
    "embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word, emb in zip(vocab,embedding_matrix):\n",
    "    token_to_embedding[word] = emb\n",
    "\n",
    "token = vocab[1000]\n",
    "print(token,token_to_embedding[token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사전 학습된 모델로 임베딩 계층 초기화 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec.load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEXT_018_230_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
