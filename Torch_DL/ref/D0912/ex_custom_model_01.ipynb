{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 상용자 정의 모델 클래스\n",
    "- 부모클래스 : nn.Module\n",
    "- 필수오버라이딩 \n",
    "    * _ _init_ _() :  모델 층 구성 즉, 설계\n",
    "    * forward() : 순방향 학습 진행 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모듈로딩\n",
    "import torch                                # 텐서 관련 모듈 \n",
    "import torch.nn as nn                       # 인공신경망 관련 모듈\n",
    "import torch.nn.functional as F             # 인공신경망 관련 함수들 모듈 ( 손실함수, 활성화함수 등등 )\n",
    "import torch.optim  as optim                # 최적화 관련 모듈 (가중치, 절편 빠르게 찾아주는 알고리즘)\n",
    "from torchinfo import summary               # 모델 구조 및 정보 관련 모듈 \n",
    "from torchmetrics.regression import *       # 회귀 성능 지표 관련 모듈\n",
    "from torchmetrics.classification import *   # 분류 성능 지표 관련 모듈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 고정\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 텐서 저장 및 실행 위치 설정\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [기본] 신경망클래스 <hr>\n",
    "    * 입력층 - 입력 : 피쳐수 고정\n",
    "    * 출력층 - 출력 : 타겟수 고정\n",
    "    * 은닉층 - 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계\n",
    "# 데이터셋 : 피쳐 4개,  타겟 1개,   회귀\n",
    "# 입 력 층 : 입력    4개     출력   20개      AF  ReLU\n",
    "# 은 닉 층 : 입력   20개     출력  100개      AF  ReLU\n",
    "# 출 력 층 : 입력  100개     출력    1개      AF  X , Sigmoid & Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식클래스의 인스턴스 속성 설정\n",
    "        self.input_layer= nn.Linear(4, 20)    # W 4 + b 1 => 1P , 5 * 20 = 100개 변수\n",
    "        self.hidden_layer=nn.Linear(20, 100)  # W 20 + b 1 => 1P , 21 * 100  = 2100개 변수\n",
    "        self.output_layer=nn.Linear(100, 1)   # W 100 + b 1 => 1P, 101 * 1 = 101개 변수\n",
    "        \n",
    "        \n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y=self.input_layer(x)           # 1개 퍼셉트론 : y=x1W1+x2W2+x3W3+x4W4+b\n",
    "        y=F.relu(y)                     # 0 <= y   ----> 죽은 relu ==> leakyReLu\n",
    "        \n",
    "        y=self.hidden_layer(y)          # 1개 퍼셉트론 : y=x1W1+x2W2+~~~+x20W20+b\n",
    "        y=F.relu(y)\n",
    "        \n",
    "        return self.output_layer(y)     # 1개 퍼셉트론 : y=x1W1+x2W2+~~~+x100W100+b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수가 동적인 모델 \n",
    "class MyModel2(nn.Module):\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self, in_features):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식클래스의 인스턴스 속성 설정\n",
    "        self.input_layer= nn.Linear(in_features, 20)    # W 4 + b 1 => 1P , 5 * 20 = 100개 변수\n",
    "        self.hidden_layer=nn.Linear(20, 100)            # W 20 + b 1 => 1P , 21 * 100  = 2100개 변수\n",
    "        self.output_layer=nn.Linear(100, 1)             # W 100 + b 1 => 1P, 101 * 1 = 101개 변수\n",
    "        \n",
    "        \n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y=self.input_layer(x)                           # 1개 퍼셉트론 : y=x1W1+x2W2+x3W3+x4W4+b\n",
    "        y=F.relu(y)                                     # 0 <= y   ----> 죽은 relu ==> leakyReLu\n",
    "        \n",
    "        y=self.hidden_layer(y)                          # 1개 퍼셉트론 : y=x1W1+x2W2+~~~+x20W20+b\n",
    "        y=F.relu(y)\n",
    "        \n",
    "        return self.output_layer(y)                     # 1개 퍼셉트론 : y=x1W1+x2W2+~~~+x100W100+b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수, 은닉층 퍼셉트론 수 동적인 모델 \n",
    "class MyModel3(nn.Module):\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self, in_features, in_out, h_out):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식클래스의 인스턴스 속성 설정\n",
    "        self.input_layer= nn.Linear(in_features, in_out)    # W 4 + b 1 => 1P , 5 * 20 = 100개 변수\n",
    "        self.hidden_layer=nn.Linear(in_out, h_out)          # W 20 + b 1 => 1P , 21 * 100  = 2100개 변수\n",
    "        self.output_layer=nn.Linear(h_out, 1)               # W 100 + b 1 => 1P, 101 * 1 = 101개 변수\n",
    "        \n",
    "        \n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y=self.input_layer(x)                               # 1개 퍼셉트론 : y=x1W1+x2W2+x3W3+x4W4+b\n",
    "        y=F.relu(y)                                         # 0 <= y   ----> 죽은 relu ==> leakyReLu\n",
    "        \n",
    "        y=self.hidden_layer(y)                              # 1개 퍼셉트론 : y=x1W1+x2W2+~~~+x20W20+b\n",
    "        y=F.relu(y)\n",
    "        \n",
    "        return self.output_layer(y)                         # 1개 퍼셉트론 : y=x1W1+x2W2+~~~+x100W100+b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은닉층의 개수가 동적인 모델\n",
    "class MyModel4(nn.Module):\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self, in_feature, out_feature, in_hiddend, hidden_cnt):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        \n",
    "        # 자식클래스의 인스턴스 속성 설정\n",
    "        self.input_layer= nn.Linear(in_feature, in_hiddend) \n",
    "        self.h1_layer=nn.ModuleList([nn.Linear(in_hiddend, in_hiddend) for _ in range(hidden_cnt)])\n",
    "        self.output_layer=nn.Linear(in_hiddend, out_feature)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel4(\n",
      "  (input_layer): Linear(in_features=5, out_features=10, bias=True)\n",
      "  (h1_layer): ModuleList(\n",
      "    (0-2): 3 x Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "input_layer.weight torch.Size([10, 5])\n",
      "input_layer.bias torch.Size([10])\n",
      "h1_layer.0.weight torch.Size([10, 10])\n",
      "h1_layer.0.bias torch.Size([10])\n",
      "h1_layer.1.weight torch.Size([10, 10])\n",
      "h1_layer.1.bias torch.Size([10])\n",
      "h1_layer.2.weight torch.Size([10, 10])\n",
      "h1_layer.2.bias torch.Size([10])\n",
      "output_layer.weight torch.Size([1, 10])\n",
      "output_layer.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "m4=MyModel4(5, 1, 10, 3)\n",
    "print(m4)\n",
    "for name, param in m4.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은닉층의 개수가 동적인 모델\n",
    "class MyModel5(nn.Module):\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self, in_in, in_out, out_out, h_ins=[], h_outs=[]):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식클래스의 인스턴스 속성 설정\n",
    "        self.input_layer= nn.Linear(in_in, h_ins[0] if len(h_ins) else in_out) \n",
    "        \n",
    "        self.h1_layer=nn.ModuleList()\n",
    "        for idx in range(len(h_ins)):\n",
    "            self.h1_layer.append(nn.Linear(h_ins[idx], h_outs[idx]))\n",
    "        self.output_layer=nn.Linear(h_outs[-1] if len(h_outs) else in_out, out_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y=F.relu(self.input_layer(x))\n",
    "        for linear in self.h1_layer:\n",
    "            y=F.relu(linear(y))\n",
    "        return self.output_layer(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel5(\n",
      "  (input_layer): Linear(in_features=3, out_features=10, bias=True)\n",
      "  (h1_layer): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=30, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "input_layer.weight torch.Size([10, 3])\n",
      "input_layer.bias torch.Size([10])\n",
      "h1_layer.0.weight torch.Size([30, 10])\n",
      "h1_layer.0.bias torch.Size([30])\n",
      "output_layer.weight torch.Size([1, 30])\n",
      "output_layer.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "#h_ins, h_outs =[10,30,50], [30,50,70]\n",
    "h_ins, h_outs =[10], [30]\n",
    "m5=MyModel5(3,10,1,h_ins, h_outs)\n",
    "print(m5)\n",
    "summary(m5)\n",
    "for name, param in m5.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1055],\n",
      "        [0.1103]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 학습 진행 ==> 모델인스턴스명(데이터)\n",
    "# 임의의 데이터 \n",
    "dataTS=torch.FloatTensor([[1,3,5], [2,4,6]])\n",
    "targetTS=torch.FloatTensor([[4],[5]])\n",
    "\n",
    "# 학습\n",
    "pre_y=m5(dataTS)\n",
    "\n",
    "print(pre_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4418],\n",
      "        [0.5492]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m5=MyModel5(5,10,1)\n",
    "# print(m5)\n",
    "# summary(m5)\n",
    "\n",
    "dataTS=torch.FloatTensor([[1,3,5,7,9], [2,4,6, 8,10]])\n",
    "targetTS=torch.FloatTensor([[4],[5]])\n",
    "# 학습\n",
    "pre_y=m5(dataTS)\n",
    "\n",
    "print(pre_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel4(\n",
      "  (input_layer): Linear(in_features=5, out_features=10, bias=True)\n",
      "  (h1_layer): ModuleList(\n",
      "    (0-2): 3 x Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "input_layer.weight torch.Size([10, 5])\n",
      "input_layer.bias torch.Size([10])\n",
      "h1_layer.0.weight torch.Size([10, 10])\n",
      "h1_layer.0.bias torch.Size([10])\n",
      "h1_layer.1.weight torch.Size([10, 10])\n",
      "h1_layer.1.bias torch.Size([10])\n",
      "h1_layer.2.weight torch.Size([10, 10])\n",
      "h1_layer.2.bias torch.Size([10])\n",
      "output_layer.weight torch.Size([1, 10])\n",
      "output_layer.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "m4=MyModel4(5, 1, 10, 3)\n",
    "print(m4)\n",
    "for name, param in m4.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
