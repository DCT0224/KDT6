{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 퍼셉트론이란 인간의 신경망을 모방하여 컴퓨터에게 학습시키기 위해 만들어진 인공신경으로 다수의 신호를 입력받아 하나의 신호를 출력하는 형태를 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. 4개의 피쳐가 하나의 퍼셉트론으로 들어오면 output이 하나인 형태인 일차함수 형태가 된다.\n",
    "\t- ( y = w1x1 + w2x2 + w3x3 + w4x4 + b )\n",
    "\n",
    "3. 입력신호를 입력받아 특정 형태의 함수를 거쳐서 출력신호로 변환하는데 이때 이 기능을 하는 함수를 활성화 함수라고 한다.\n",
    "\n",
    "4. \t\n",
    "- 시그모이드(sigmoid) 함수 : ( 0 ~ 1 사이) 실수 값을 입력받아 큰 음수 일수록 0에 가까워지고 큰 양수 일수록 1에 가까워짐. 그러나 기울기 소실 문제가 발생 가능\n",
    "- 탄젠트(tanh) 함수 : ( -1 ~ 1  사이)  시그모이드와 유사하지만 실수값을 입력받아 -1 ~ 1사이 값을 출력함. 여전히 기울기 소실 문제 가지고 있음.\n",
    "- ReLU 함수 : ( 0 ~ x 사이) , 입력값이 0 이하면 0을 출력 입력값이 0 이상이면 그대로 출력, 그러나 죽은 뉴런의 발생 가능성 존재\n",
    "- Leaky ReLU : (ax ~ x 사이), 죽은 뉴런의 발생 가능성을 최소화하기 위해 입력값이 음수라도 기울기가 사라지는 것을 방지함.\n",
    "\n",
    "5. \t딥러닝 알고리즘의 학습 목표는 예측값과 정답값 간의 차이인 손실 함수의 크기를 최소화시키는 것. 이를 위해 손실 함\n",
    "\t수 값의 변화에 따라 가중치 혹은 편항을 업데이트해야 한다. x축에 가중치 값을 y축에 손실함수를 두고 최적의 가중치를 \t찾기 위한 과정을 경사하강법이라고 한다.\n",
    "\t종류는 크게 BDG, SGD, MGD가 있는데 하나의 epoch에서 전체 데이터 세트를 다루면 BDG, epoch 안에 하나의 임의의 \t데이터를 다루면 SGD, 설정한 미니 배치 사이즈 만큼 다루면 MGD이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 피쳐 3개\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class RegModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_layer = nn.Linear(3,30)\n",
    "        self.hd_layer = nn.Linear(30,10)\n",
    "        self.out_layer = nn.Linear(10,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = F.relu(self.in_layer(x))\n",
    "        y = F.relu(self.hd_layer(y))\n",
    "        return self.out_layer(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 피쳐 5개, 클래스 2개, 층 4개\n",
    "class BCFModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_layer=nn.Linear(5,20)\n",
    "        self.hd1_layer=nn.Linear(20,10)\n",
    "        self.hd2_layer=nn.Linear(10,5)\n",
    "        self.out_layer=nn.Linear(5,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = F.relu(self.in_layer(x))\n",
    "        y = F.relu(self.hd1_layer(y))\n",
    "        y = F.relu(self.hd2_layer(y))\n",
    "        return F.sigmoid(self.out_layer(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 피쳐 5개, 클래스 8개, 층 3~5개, 퍼셉트론 동적\n",
    "class MCFModel1(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_layer=nn.Linear(5,20)\n",
    "        self.hd1_layer=nn.Linear(20,10)\n",
    "        self.out_layer=nn.Linear(10,8)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = F.relu(self.in_layer(x))\n",
    "        y = F.relu(self.hd1_layer(y))\n",
    "        return self.out_layer(y)\n",
    "    \n",
    "class MCFModel2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_layer=nn.Linear(5,30)\n",
    "        self.hd1_layer=nn.Linear(30,20)\n",
    "        self.hd2_layer=nn.Linear(20,10)\n",
    "        self.out_layer=nn.Linear(10,8)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = F.relu(self.in_layer(x))\n",
    "        y = F.relu(self.hd1_layer(y))\n",
    "        y = F.relu(self.hd2_layer(y))\n",
    "        return self.out_layer(y)\n",
    "\n",
    "class MCFModel3(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_layer=nn.Linear(5,30)\n",
    "        self.hd1_layer=nn.Linear(30,20)\n",
    "        self.hd2_layer=nn.Linear(20,10)\n",
    "        self.hd3_layer=nn.Linear(10,10)\n",
    "        self.out_layer=nn.Linear(10,8)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = F.relu(self.in_layer(x))\n",
    "        y = F.relu(self.hd1_layer(y))\n",
    "        y = F.relu(self.hd2_layer(y))\n",
    "        y = F.relu(self.hd3_layer(y))\n",
    "        return self.out_layer(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[9] Sigmoid 함수와 tanh 함수에서 역전파를 통하여 기울기를 구해야하는데 이때 layer층이 너무 많아 미분값이 거듭 곱해지게 되면 기울기가 점차 0으로 수렴하게 되는 기울기 소실 문제가 생기게 된다. tanh함수는 Sigmoid함수보다 범위를 두배 늘렸다 하더라도 이 문제를 해결하지 못함. 이를 위해 ReLU 함수를 활성화 함수로 사용하게 되는데 ReLU 또한 죽은 뉴런이라는 문제가 존재하기 때문에 죽은 뉴런을 보완하기 위해 Leaky ReLU 함수를 사용하기도 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[10] \n",
    "- model.train() : 모델이 학숩하기 위해 호출되는 함수, 모델 파라미터 업데이트 및 기울기 계산\n",
    "드롭아웃 및 배치 정규화, 기울기 계산, 파라미터 최적화 업데이트가 가능하도록 함\n",
    "- model.eval() : 모델을 평가 모드로 전환하여 모델 파라미터들이 업데이트 되는 것을 방지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
