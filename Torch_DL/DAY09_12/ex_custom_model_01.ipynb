{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용자 정의 모델 클래스\n",
    "- 부모 클래스 : nn.Module\n",
    "- 필수 오버라이딩 \n",
    "    *  __init__() : 모델 층 구성 즉, 설계\n",
    "    * forward() : 순방향 학습 진행 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모듈 로딩\n",
    "import torch                     # 텐서 관련 모듈\n",
    "import torch.nn as nn            # 인공신경망 모듈\n",
    "import torch.nn.functional as F  # 인공신경망 관련 함수들 모듈(손실함수, 활성화함수 등등)\n",
    "import torch.optim as optim      # 최적화 관련 모듈( 가중치, 절편 빠르게 찾아주는 알고리즘)\n",
    "from torchinfo import summary    # 모델 구조 및 정보 관련 모듈\n",
    "from torchmetrics.regression import * # *=> regression안에 있는 애들 다 들고옴\n",
    "# 회귀 성능 지표 관련 모듈\n",
    "from torchmetrics.classification import *\n",
    "# 분류 성능 지표 관련 모듈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 고정\n",
    "torch.manual_seed(7)\n",
    "\n",
    "# 텐서 저장 및 실행 위치 설정\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [기본] 신경망클래스 <hr>\n",
    "    * 입력층 - 입력 : 피쳐수 고정\n",
    "    * 출력층 - 출력 : 타겟수 고정\n",
    "    * 은닉층 - 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계\n",
    "# 데이터 셋 : 피쳐 4개, 타겟 1개, 회귀\n",
    "# 입력층 : 입력  4개   출력  20개    AF  ReLU\n",
    "# 은닉층 : 입력  20개  출력  100개   AF  ReLU\n",
    "# 출력층 : 입력  100개 출력  1개     AF  X,                                      Sigmoid & Softmax(다중)\n",
    "#                                                                              (분류에서는 확률로 변환해서 줌)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 상속\n",
    "class MyModel(nn.Module):\n",
    "    # __ooo__ : 매직 메서드 : 파이썬 시스템에 의해 호출되는 메서드\n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self):\n",
    "        # 부모 클래서 먼저 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(4,20)     # W 4 + b 1 => 1P, 5 * 20 = 100개 변수\n",
    "        self.hidden_layer = nn.Linear(20,100)  # W 20 + b 1 => 1P, 21 * 100 = 2100개 변수\n",
    "        self.output_layer = nn.Linear(100,1)   # W 100 + b 1 => 1P , 101 * 1 = 101개 변수\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self,x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x) # 1개 퍼셉트론 : y = x1w1 + x2w2 + x3w3 + x4w4 + b\n",
    "        y = F.relu(y) # 엑티베이션 함수 거쳐서 결과값 냄\n",
    "        #   0 <= y  -----> 죽은 relu ==> leakyReLu\n",
    "\n",
    "        y = self.hidden_layer(y) # 1개 퍼셉트론 : y = x1w1 + x2w2 ****+ x19w19 + x20w20 + b\n",
    "        y = F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) # 1개 퍼셉트론 : y = x1w1 + x2w2 ****+ x99w99 + x100w100 + b\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수가 동적인 모델\n",
    "class MyModel2(nn.Module):\n",
    "    # __ooo__ : 매직 메서드 : 파이썬 시스템에 의해 호출되는 메서드\n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self,in_feature):\n",
    "        # 부모 클래서 먼저 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(in_feature,20)     # W 4 + b 1 => 1P, 5 * 20 = 100개 변수\n",
    "        self.hidden_layer = nn.Linear(20,100)  # W 20 + b 1 => 1P, 21 * 100 = 2100개 변수\n",
    "        self.output_layer = nn.Linear(100,1)   # W 100 + b 1 => 1P , 101 * 1 = 101개 변수\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self,x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x) # 1개 퍼셉트론 : y = x1w1 + x2w2 + x3w3 + x4w4 + b\n",
    "        y = F.relu(y) # 엑티베이션 함수 거쳐서 결과값 냄\n",
    "        #   0 <= y  -----> 죽은 relu ==> leakyReLu\n",
    "\n",
    "        y = self.hidden_layer(y) # 1개 퍼셉트론 : y = x1w1 + x2w2 ****+ x19w19 + x20w20 + b\n",
    "        y = F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) # 1개 퍼셉트론 : y = x1w1 + x2w2 ****+ x99w99 + x100w100 + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수, 은닉츠 퍼셉트론 수 동적인 모델\n",
    "class MyModel3(nn.Module):\n",
    "    # __ooo__ : 매직 메서드 : 파이썬 시스템에 의해 호출되는 메서드\n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self,in_feature,in_out,hid_out):\n",
    "        # 부모 클래서 먼저 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(in_feature,in_out)     # W 4 + b 1 => 1P, 5 * 20 = 100개 변수\n",
    "        self.hidden_layer = nn.Linear(in_out,hid_out)  # W 20 + b 1 => 1P, 21 * 100 = 2100개 변수\n",
    "        self.output_layer = nn.Linear(hid_out,1)   # W 100 + b 1 => 1P , 101 * 1 = 101개 변수\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self,x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x) # 1개 퍼셉트론 : y = x1w1 + x2w2 + x3w3 + x4w4 + b\n",
    "        y = F.relu(y) # 엑티베이션 함수 거쳐서 결과값 냄\n",
    "        #   0 <= y  -----> 죽은 relu ==> leakyReLu\n",
    "\n",
    "        y = self.hidden_layer(y) # 1개 퍼셉트론 : y = x1w1 + x2w2 ****+ x19w19 + x20w20 + b\n",
    "        y = F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) # 1개 퍼셉트론 : y = x1w1 + x2w2 ****+ x99w99 + x100w100 + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은닉층의 개수가 동적인 모델\n",
    "class MyModel4(nn.Module):\n",
    "    # __ooo__ : 매직 메서드 : 파이썬 시스템에 의해 호출되는 메서드\n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self,in_feature,in_out,hid_out,hid_cnt):\n",
    "        # 부모 클래서 먼저 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(in_feature,in_out)     # W 4 + b 1 => 1P, 5 * 20 = 100개 변수\n",
    "        for i in range(hid_cnt):\n",
    "            \n",
    "            self.hidden_layer = nn.Linear(in_out,hid_out)  # W 20 + b 1 => 1P, 21 * 100 = 2100개 변수\n",
    "            \n",
    "        self.output_layer = nn.Linear(hid_out,1)   # W 100 + b 1 => 1P , 101 * 1 = 101개 변수\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self,x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x) # 1개 퍼셉트론 : y = x1w1 + x2w2 + x3w3 + x4w4 + b\n",
    "        y = F.relu(y) # 엑티베이션 함수 거쳐서 결과값 냄\n",
    "        #   0 <= y  -----> 죽은 relu ==> leakyReLu\n",
    "\n",
    "        y = self.hidden_layer(y) # 1개 퍼셉트론 : y = x1w1 + x2w2 ****+ x19w19 + x20w20 + b\n",
    "        y = F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) # 1개 퍼셉트론 : y = x1w1 + x2w2 ****+ x99w99 + x100w100 + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel4(\n",
       "  (input_layer): Linear(in_features=3, out_features=50, bias=True)\n",
       "  (hidden_layer): Linear(in_features=50, out_features=30, bias=True)\n",
       "  (output_layer): Linear(in_features=30, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 모델 인스턴스 생성\n",
    "m1=MyModel4(3,50,30,5)\n",
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('input_layer.weight', Parameter containing:\n",
      "tensor([[ 0.0403, -0.3478,  0.1838],\n",
      "        [ 0.1812, -0.3086, -0.0865],\n",
      "        [-0.3382,  0.1498, -0.1555],\n",
      "        [ 0.4056,  0.4099,  0.0588],\n",
      "        [-0.2461, -0.3391, -0.0634],\n",
      "        [-0.1625,  0.2545, -0.4930],\n",
      "        [ 0.5426, -0.4529,  0.4421],\n",
      "        [-0.1003,  0.2970,  0.2250],\n",
      "        [ 0.0242,  0.1077,  0.4384],\n",
      "        [ 0.1485,  0.3063, -0.4466],\n",
      "        [ 0.4110,  0.1987,  0.1462],\n",
      "        [ 0.0798,  0.2814,  0.5303],\n",
      "        [-0.1285, -0.3216, -0.1453],\n",
      "        [-0.3519,  0.2777, -0.2853],\n",
      "        [-0.3081,  0.4982,  0.5283],\n",
      "        [ 0.0664, -0.1000, -0.0745],\n",
      "        [ 0.2735, -0.5391, -0.4718],\n",
      "        [ 0.4612,  0.5700, -0.0343],\n",
      "        [-0.4562,  0.0158, -0.2686],\n",
      "        [-0.0011,  0.2826,  0.2556],\n",
      "        [-0.0677,  0.0636,  0.1571],\n",
      "        [-0.4525, -0.1957,  0.0226],\n",
      "        [-0.3294, -0.2521,  0.1939],\n",
      "        [ 0.3323,  0.0081, -0.2246],\n",
      "        [ 0.3161, -0.3165, -0.1355],\n",
      "        [ 0.5447,  0.1063,  0.0407],\n",
      "        [ 0.1946, -0.1408,  0.4940],\n",
      "        [-0.4709, -0.1588, -0.0998],\n",
      "        [-0.2376, -0.1429,  0.3578],\n",
      "        [ 0.3168,  0.0321,  0.5030],\n",
      "        [-0.4076, -0.3443,  0.2609],\n",
      "        [-0.0310, -0.5261, -0.3996],\n",
      "        [-0.3515,  0.4458,  0.2615],\n",
      "        [ 0.4466, -0.4298, -0.5629],\n",
      "        [-0.3786,  0.0618, -0.1140],\n",
      "        [-0.3638,  0.4203, -0.1862],\n",
      "        [-0.3339,  0.2667, -0.1556],\n",
      "        [ 0.3681,  0.1376,  0.0142],\n",
      "        [ 0.1212, -0.4338, -0.3089],\n",
      "        [-0.2714, -0.3267,  0.1024],\n",
      "        [ 0.1153, -0.2399,  0.3764],\n",
      "        [-0.3212,  0.1166, -0.1649],\n",
      "        [-0.5008, -0.3759,  0.2358],\n",
      "        [ 0.2285, -0.3570, -0.1800],\n",
      "        [-0.5397,  0.3104,  0.4992],\n",
      "        [ 0.1888, -0.3572,  0.3045],\n",
      "        [ 0.4791, -0.1181, -0.0946],\n",
      "        [ 0.1998,  0.0743, -0.1908],\n",
      "        [-0.5360, -0.4048,  0.0313],\n",
      "        [-0.0310,  0.4768,  0.0198]], requires_grad=True))\n",
      "('input_layer.bias', Parameter containing:\n",
      "tensor([-0.1487, -0.5641, -0.5308,  0.2398,  0.5142, -0.0088, -0.5160,  0.2050,\n",
      "         0.4702, -0.3821, -0.1520,  0.0486, -0.3293, -0.5353,  0.2528, -0.4738,\n",
      "        -0.1449, -0.3066,  0.4342,  0.1115,  0.2677, -0.3910,  0.4865,  0.0514,\n",
      "        -0.3202, -0.0378, -0.4088, -0.0267,  0.3224, -0.2595,  0.4880,  0.1763,\n",
      "        -0.1010,  0.1804, -0.1064, -0.1342, -0.3788,  0.3364,  0.3466,  0.1977,\n",
      "         0.3415,  0.5768,  0.5436, -0.5572,  0.1316, -0.0494,  0.1960, -0.1120,\n",
      "         0.1611, -0.5312], requires_grad=True))\n",
      "('hidden_layer.weight', Parameter containing:\n",
      "tensor([[ 0.0646,  0.1359, -0.0254,  ..., -0.1284, -0.0323,  0.1281],\n",
      "        [ 0.0341, -0.0089, -0.0404,  ...,  0.1347, -0.0655,  0.0905],\n",
      "        [ 0.1235,  0.0146, -0.0486,  ...,  0.0692, -0.0917,  0.1110],\n",
      "        ...,\n",
      "        [ 0.1137,  0.0969, -0.0511,  ...,  0.0877,  0.0330,  0.0727],\n",
      "        [ 0.1077,  0.0605,  0.0569,  ..., -0.0894,  0.0645, -0.0011],\n",
      "        [-0.0073, -0.0450, -0.0990,  ..., -0.0717, -0.1299, -0.1042]],\n",
      "       requires_grad=True))\n",
      "('hidden_layer.bias', Parameter containing:\n",
      "tensor([ 1.0419e-01, -7.6492e-02, -8.9709e-02, -7.6793e-02,  9.4249e-03,\n",
      "        -3.4307e-02, -1.1230e-01, -8.6535e-02, -9.7900e-02,  3.0276e-02,\n",
      "         1.4648e-05,  1.2102e-01,  5.4387e-02,  2.8341e-02,  1.1936e-01,\n",
      "         2.7609e-02, -6.8820e-02,  1.1751e-01, -6.4980e-02, -9.5292e-02,\n",
      "         1.0925e-01,  1.2591e-01,  8.0337e-02, -3.3539e-02,  1.3218e-01,\n",
      "         7.3690e-02,  2.0703e-02,  1.1486e-02,  6.8275e-02, -6.7302e-02],\n",
      "       requires_grad=True))\n",
      "('output_layer.weight', Parameter containing:\n",
      "tensor([[-0.0636,  0.1033,  0.0238, -0.0100, -0.0142,  0.0079,  0.1810,  0.1575,\n",
      "         -0.1720, -0.1421, -0.1404, -0.0069,  0.0451, -0.1014,  0.0817, -0.1782,\n",
      "          0.1272, -0.0043,  0.0571,  0.1277,  0.0197, -0.0894, -0.0510,  0.1635,\n",
      "         -0.0372, -0.0085, -0.1533, -0.1143, -0.0107,  0.0473]],\n",
      "       requires_grad=True))\n",
      "('output_layer.bias', Parameter containing:\n",
      "tensor([-0.1114], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "## 모델 파라미터 즉, W와 b 확인\n",
    "for m in m1.named_parameters() : print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling forward()\n",
      "tensor([[-0.4332],\n",
      "        [-0.4814]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 학습 진행 ==> 모델 인스턴스명(데이터)\n",
    "# 임의의 데이터\n",
    "dataTS = torch.FloatTensor([[1,3,5],[2,4,6]])\n",
    "targetTS = torch.FloatTensor([[4],[5]])\n",
    "\n",
    "# 학습\n",
    "pre_y = m1(dataTS)\n",
    "\n",
    "print(pre_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
